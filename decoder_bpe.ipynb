{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e893663-462c-4e19-98f0-2611f454ccf7",
    "outputId": "96f88eb4-3db5-4384-94ad-74718a1b50fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\satvm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "46a99932-f65a-4ffd-981b-4eb2c1c32775"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"poetry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "259e65ac-45c6-452d-b220-6f9e3f605093",
    "outputId": "1d03b069-37f4-4e1d-9199-71d98240230c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7,\n        \"min\": 0,\n        \"max\": 15,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          14,\n          1,\n          15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\r\\r\\n                    The Death of Atahuallpa\\r\\r\\n                \",\n          \"\\r\\r\\n                    !\\r\\r\\n                \",\n          \"\\r\\r\\n                    Poet's Wish\\r\\r\\n                \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Poem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"\\r\\r\\n\\r\\r\\n\",\n          \"\\r\\r\\nDear Writers, I\\u2019m compiling the first in what I hope is a series of publications I\\u2019m calling artists among artists. The theme for issue 1 is \\u201cFaggot Dinosaur.\\u201d I hope to hear from you! Thank you and best wishes.  \",\n          \"\\r\\r\\nWe'd  like  to  talk  with  you  about  fear they  said  so\\r\\r\\r\\nmany  people  live  in  fear  these  days  they  drove  up\\r\\r\\r\\nall  four  of  them  in  a  small  car nice   boy  they  said\\r\\r\\r\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\r\\r\\r\\nof  the  woman  the other  two  waiting  in  the  drive  I\\r\\r\\r\\nwas  outside digging up the garden no one home I said\\r\\r\\r\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\r\\r\\r\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\r\\r\\r\\ncard  there's   a  phone  number  you  can  call  anytime\\r\\r\\r\\nany  other   houses  down  this  road  anyone  else   live\\r\\r\\r\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\\r\\r\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Poet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"William Jay Smith\",\n          \"Wendy Videlock\",\n          \"Jody Gladding\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Living,Social Commentaries,Popular Culture\",\n          \"Arts & Sciences,Philosophy\",\n          \"Relationships,Gay, Lesbian, Queer,Arts & Sciences,Poetry & Poets,Social Commentaries,Gender & Sexuality\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-15c9ac30-b725-4b51-87b0-fb22b068f355\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>13</td>\n",
       "      <td>\\r\\r\\n                    1-800-FEAR\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nWe'd  like  to  talk  with  you  about  fear they  said  so\\r\\r\\r\\nmany  people  live  in  fear  these  days  they  drove  up\\r\\r\\r\\nall  four  of  them  in  a  small  car nice   boy  they  said\\r\\r\\r\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\r\\r\\r\\nof  the  woman  the other  two  waiting  in  the  drive  I\\r\\r\\r\\nwas  outside digging up the garden no one home I said\\r\\r\\r\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\r\\r\\r\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\r\\r\\r\\ncard  there's   a  phone  number  you  can  call  anytime\\r\\r\\r\\nany  other   houses  down  this  road  anyone  else   live\\r\\r\\r\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\\r\\r\\n</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>Living,Social Commentaries,Popular Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td>14</td>\n",
       "      <td>\\r\\r\\n                    The Death of Atahuallpa\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\n\\r\\r\\n</td>\n",
       "      <td>William Jay Smith</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13851</th>\n",
       "      <td>15</td>\n",
       "      <td>\\r\\r\\n                    Poet's Wish\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\n\\r\\r\\n</td>\n",
       "      <td>William Jay Smith</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\r\\n                    0\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\n          Philosophic\\r\\r\\nin its complex, ovoid emptiness,\\r\\r\\na skillful pundit coined it as a sort\\r\\r\\n    of stopgap doorstop for those\\r\\r\\n           quaint equations           Romans never\\r\\r\\ndreamt of. In form completely clever\\r\\r\\nand discrete—a mirror come unsilvered,     loose watch face without the works,              a hollowed globe            from tip to toe\\r\\r\\nunbroken, it evades the grappling\\r\\r\\nhooks of mass, tilts the thin rim of no thing,     remains embryonic sum,             non-cogito.\\r\\r\\n</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>Arts &amp; Sciences,Philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\r\\n                    !\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nDear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Sciences,Poetry &amp; Poets,Social Commentaries,Gender &amp; Sexuality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15c9ac30-b725-4b51-87b0-fb22b068f355')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-15c9ac30-b725-4b51-87b0-fb22b068f355 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-15c9ac30-b725-4b51-87b0-fb22b068f355');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1264de61-db49-4f96-86ac-7f8c3099238d\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1264de61-db49-4f96-86ac-7f8c3099238d')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1264de61-db49-4f96-86ac-7f8c3099238d button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       Unnamed: 0  \\\n",
       "13849          13   \n",
       "13850          14   \n",
       "13851          15   \n",
       "13852           0   \n",
       "13853           1   \n",
       "\n",
       "                                                                         Title  \\\n",
       "13849               \\r\\r\\n                    1-800-FEAR\\r\\r\\n                   \n",
       "13850  \\r\\r\\n                    The Death of Atahuallpa\\r\\r\\n                   \n",
       "13851              \\r\\r\\n                    Poet's Wish\\r\\r\\n                   \n",
       "13852                        \\r\\r\\n                    0\\r\\r\\n                   \n",
       "13853                        \\r\\r\\n                    !\\r\\r\\n                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Poem  \\\n",
       "13849  \\r\\r\\nWe'd  like  to  talk  with  you  about  fear they  said  so\\r\\r\\r\\nmany  people  live  in  fear  these  days  they  drove  up\\r\\r\\r\\nall  four  of  them  in  a  small  car nice   boy  they  said\\r\\r\\r\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\r\\r\\r\\nof  the  woman  the other  two  waiting  in  the  drive  I\\r\\r\\r\\nwas  outside digging up the garden no one home I said\\r\\r\\r\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\r\\r\\r\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\r\\r\\r\\ncard  there's   a  phone  number  you  can  call  anytime\\r\\r\\r\\nany  other   houses  down  this  road  anyone  else   live\\r\\r\\r\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\\r\\r\\n   \n",
       "13850                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\r\\r\\n\\r\\r\\n   \n",
       "13851                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\r\\r\\n\\r\\r\\n   \n",
       "13852                                                                                                                                                                                                         \\r\\r\\n          Philosophic\\r\\r\\nin its complex, ovoid emptiness,\\r\\r\\na skillful pundit coined it as a sort\\r\\r\\n    of stopgap doorstop for those\\r\\r\\n           quaint equations           Romans never\\r\\r\\ndreamt of. In form completely clever\\r\\r\\nand discrete—a mirror come unsilvered,     loose watch face without the works,              a hollowed globe            from tip to toe\\r\\r\\nunbroken, it evades the grappling\\r\\r\\nhooks of mass, tilts the thin rim of no thing,     remains embryonic sum,             non-cogito.\\r\\r\\n   \n",
       "13853                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\r\\r\\nDear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.     \n",
       "\n",
       "                    Poet  \\\n",
       "13849      Jody Gladding   \n",
       "13850  William Jay Smith   \n",
       "13851  William Jay Smith   \n",
       "13852  Hailey Leithauser   \n",
       "13853     Wendy Videlock   \n",
       "\n",
       "                                                                                                          Tags  \n",
       "13849                                                               Living,Social Commentaries,Popular Culture  \n",
       "13850                                                                                                      NaN  \n",
       "13851                                                                                                      NaN  \n",
       "13852                                                                               Arts & Sciences,Philosophy  \n",
       "13853  Relationships,Gay, Lesbian, Queer,Arts & Sciences,Poetry & Poets,Social Commentaries,Gender & Sexuality  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "68c8a2b5-553f-494e-bed7-b87dda4a31a0",
    "outputId": "f585a655-b3bb-47bb-cd1a-b220d06c67a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "Title           0\n",
       "Poem            0\n",
       "Poet            0\n",
       "Tags          955\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bfb30a34-a0f3-4202-b652-931b29a215a1"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'Tags', 'Poet', 'Title'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "744c8e4b-7444-4c88-bceb-d1986680a5da"
   },
   "outputs": [],
   "source": [
    "# df['Title'] = df['Title'].str.replace('\\r', '')\n",
    "df['Poem'] = df['Poem'].str.replace('\\r', '')\n",
    "# df['Title'] = df['Title'].str.replace('\\n', '')\n",
    "df['Poem'] = df['Poem'].str.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "XHKDr0NhuaQU",
    "outputId": "b58474e3-1194-4563-e6c3-80a0aca43ee4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Poem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"\",\n          \"Dear Writers, I\\u2019m compiling the first in what I hope is a series of publications I\\u2019m calling artists among artists. The theme for issue 1 is \\u201cFaggot Dinosaur.\\u201d I hope to hear from you! Thank you and best wishes.  \",\n          \"We'd  like  to  talk  with  you  about  fear they  said  so\\nmany  people  live  in  fear  these  days  they  drove  up\\nall  four  of  them  in  a  small  car nice   boy  they  said\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\nof  the  woman  the other  two  waiting  in  the  drive  I\\nwas  outside digging up the garden no one home I said\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\ncard  there's   a  phone  number  you  can  call  anytime\\nany  other   houses  down  this  road  anyone  else   live\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b737067f-5288-46d2-a788-98f3a10ef199\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>We'd  like  to  talk  with  you  about  fear they  said  so\\nmany  people  live  in  fear  these  days  they  drove  up\\nall  four  of  them  in  a  small  car nice   boy  they  said\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\nof  the  woman  the other  two  waiting  in  the  drive  I\\nwas  outside digging up the garden no one home I said\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\ncard  there's   a  phone  number  you  can  call  anytime\\nany  other   houses  down  this  road  anyone  else   live\\nhere  we'd  like  to  talk  to  them  about  living  in  fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13851</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>Philosophic\\nin its complex, ovoid emptiness,\\na skillful pundit coined it as a sort\\n    of stopgap doorstop for those\\n           quaint equations           Romans never\\ndreamt of. In form completely clever\\nand discrete—a mirror come unsilvered,     loose watch face without the works,              a hollowed globe            from tip to toe\\nunbroken, it evades the grappling\\nhooks of mass, tilts the thin rim of no thing,     remains embryonic sum,             non-cogito.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>Dear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b737067f-5288-46d2-a788-98f3a10ef199')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b737067f-5288-46d2-a788-98f3a10ef199 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b737067f-5288-46d2-a788-98f3a10ef199');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c4407e73-1286-47cd-ae75-4807ac9f28bc\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c4407e73-1286-47cd-ae75-4807ac9f28bc')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c4407e73-1286-47cd-ae75-4807ac9f28bc button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Poem\n",
       "13849  We'd  like  to  talk  with  you  about  fear they  said  so\\nmany  people  live  in  fear  these  days  they  drove  up\\nall  four  of  them  in  a  small  car nice   boy  they  said\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\nof  the  woman  the other  two  waiting  in  the  drive  I\\nwas  outside digging up the garden no one home I said\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\ncard  there's   a  phone  number  you  can  call  anytime\\nany  other   houses  down  this  road  anyone  else   live\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\n",
       "13850                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "13851                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "13852                                                                                                                                                                                       Philosophic\\nin its complex, ovoid emptiness,\\na skillful pundit coined it as a sort\\n    of stopgap doorstop for those\\n           quaint equations           Romans never\\ndreamt of. In form completely clever\\nand discrete—a mirror come unsilvered,     loose watch face without the works,              a hollowed globe            from tip to toe\\nunbroken, it evades the grappling\\nhooks of mass, tilts the thin rim of no thing,     remains embryonic sum,             non-cogito.\n",
       "13853                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Dear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "147cacee-0272-4883-a8b0-5e9f9cc42621"
   },
   "outputs": [],
   "source": [
    "# Lower case everything\n",
    "# df['Title'] = df['Title'].str.lower()\n",
    "df['Poem'] = df['Poem'].str.lower()\n",
    "\n",
    "# Remove apostrophes and join the parts\n",
    "# df['Title'] = df['Title'].str.replace(\"'\", \"\", regex=False).str.replace(\"’\", \"\", regex=False)\n",
    "df['Poem'] = df['Poem'].str.replace(\"'\", \"\", regex=False).str.replace(\"’\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "d53c2016-cbe5-4bcf-9a11-94f7abf27a6a"
   },
   "outputs": [],
   "source": [
    "# Uncomment only when including encoder, until then let it be commented off\n",
    "# Filter out rows where both 'Title' and 'Poem' have no alphabetic characters\n",
    "\n",
    "# df = df[df.apply(lambda x: any(c.isalpha() for c in x['Title']) and\n",
    "#                             any(c.isalpha() for c in x['Poem']), axis=1)]\n",
    "\n",
    "# Filter out rows with no value (its not NaN its '' in this dataset)\n",
    "df = df[df.apply(lambda x: any(c.isalpha() for c in x['Poem']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54d2c668-0a97-4e00-8976-f5260fb83bd7",
    "outputId": "de90cac7-0b17-45ec-d9ae-46c21a88a93d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-643c636f129a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Poem'] = 'START ' + df['Poem'] + ' END'\n",
      "<ipython-input-10-643c636f129a>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Poem'] = df['Poem'].apply(lambda x: re.sub(r'\\n', ' N ', x))\n"
     ]
    }
   ],
   "source": [
    "# Make newline a special token\n",
    "# df['Title'] = 'START ' + df['Title'] + ' END'\n",
    "df['Poem'] = 'START ' + df['Poem'] + ' END'\n",
    "\n",
    "df['Poem'] = df['Poem'].apply(lambda x: re.sub(r'\\n', ' N ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "255a982a-7b39-49eb-814f-c56e15fc6b44"
   },
   "outputs": [],
   "source": [
    "def remove_numeric_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    # Filter tokens to exclude any that contain digits\n",
    "    filtered_tokens = [token for token in tokens if not re.search(r'\\d', token)]\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "df['Poem'] = df['Poem'].apply(remove_numeric_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605
    },
    "id": "HuEV8lK0uaQV",
    "outputId": "a5c549af-cf15-408e-c308-eb9da816291a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Poem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"START the wise men will unlearn your name . N above your head no star will flame . N one weary sound will be the same\\u2014 N the hoarse roar of the gale . N the shadows fall from your tired eyes N as your lone bedside candle dies , N for here the calendar breeds nights N till stores of candles fail . N what prompts this melancholy key ? N a long familiar melody . N it sounds again . so let it be . N let it sound from this night . N let it sound in my hour of death\\u2014 N as gratefulness of eyes and lips N for that which sometimes makes us lift N our gaze to the far sky . N you glare in silence at the wall . N your stocking gapes : no gifts at all . N its clear that you are now too old N to trust in good saint nick ; N that its too late for miracles . N \\u2014but suddenly , lifting your eyes N to heavens light , you realize : N your life is a sheer gift . END\",\n          \"START philosophic N in its complex , ovoid emptiness , N a skillful pundit coined it as a sort N of stopgap doorstop for those N quaint equations romans never N dreamt of . in form completely clever N and discrete\\u2014a mirror come unsilvered , loose watch face without the works , a hollowed globe from tip to toe N unbroken , it evades the grappling N hooks of mass , tilts the thin rim of no thing , remains embryonic sum , non-cogito . END\",\n          \"START dear writers , im compiling the first in what i hope is a series of publications im calling artists among artists . the theme for issue is \\u201c faggot dinosaur. \\u201d i hope to hear from you ! thank you and best wishes . END\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-5bd05742-d692-41dc-a762-b419155b9ab1\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>START dear writers , im compiling the first in what i hope is a series of publications im calling artists among artists . the theme for issue is “ faggot dinosaur. ” i hope to hear from you ! thank you and best wishes . END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>START the wise men will unlearn your name . N above your head no star will flame . N one weary sound will be the same— N the hoarse roar of the gale . N the shadows fall from your tired eyes N as your lone bedside candle dies , N for here the calendar breeds nights N till stores of candles fail . N what prompts this melancholy key ? N a long familiar melody . N it sounds again . so let it be . N let it sound from this night . N let it sound in my hour of death— N as gratefulness of eyes and lips N for that which sometimes makes us lift N our gaze to the far sky . N you glare in silence at the wall . N your stocking gapes : no gifts at all . N its clear that you are now too old N to trust in good saint nick ; N that its too late for miracles . N —but suddenly , lifting your eyes N to heavens light , you realize : N your life is a sheer gift . END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>START wed like to talk with you about fear they said so N many people live in fear these days they drove up N all four of them in a small car nice boy they said N beautiful dogs they said so friendly the man ahead N of the woman the other two waiting in the drive i N was outside digging up the garden no one home i said N what are you selling anyway im not interested i N said well you have a nice day they said heres our N card theres a phone number you can call anytime N any other houses down this road anyone else live N here wed like to talk to them about living in fear END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>START philosophic N in its complex , ovoid emptiness , N a skillful pundit coined it as a sort N of stopgap doorstop for those N quaint equations romans never N dreamt of . in form completely clever N and discrete—a mirror come unsilvered , loose watch face without the works , a hollowed globe from tip to toe N unbroken , it evades the grappling N hooks of mass , tilts the thin rim of no thing , remains embryonic sum , non-cogito . END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>START dear writers , im compiling the first in what i hope is a series of publications im calling artists among artists . the theme for issue is “ faggot dinosaur. ” i hope to hear from you ! thank you and best wishes . END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5bd05742-d692-41dc-a762-b419155b9ab1')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-5bd05742-d692-41dc-a762-b419155b9ab1 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-5bd05742-d692-41dc-a762-b419155b9ab1');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-997cde3a-237f-4eb7-8fe7-d59ed305a4b6\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-997cde3a-237f-4eb7-8fe7-d59ed305a4b6')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-997cde3a-237f-4eb7-8fe7-d59ed305a4b6 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Poem\n",
       "13835                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            START dear writers , im compiling the first in what i hope is a series of publications im calling artists among artists . the theme for issue is “ faggot dinosaur. ” i hope to hear from you ! thank you and best wishes . END\n",
       "13848  START the wise men will unlearn your name . N above your head no star will flame . N one weary sound will be the same— N the hoarse roar of the gale . N the shadows fall from your tired eyes N as your lone bedside candle dies , N for here the calendar breeds nights N till stores of candles fail . N what prompts this melancholy key ? N a long familiar melody . N it sounds again . so let it be . N let it sound from this night . N let it sound in my hour of death— N as gratefulness of eyes and lips N for that which sometimes makes us lift N our gaze to the far sky . N you glare in silence at the wall . N your stocking gapes : no gifts at all . N its clear that you are now too old N to trust in good saint nick ; N that its too late for miracles . N —but suddenly , lifting your eyes N to heavens light , you realize : N your life is a sheer gift . END\n",
       "13849                                                                                                                                                                                                                                                                                       START wed like to talk with you about fear they said so N many people live in fear these days they drove up N all four of them in a small car nice boy they said N beautiful dogs they said so friendly the man ahead N of the woman the other two waiting in the drive i N was outside digging up the garden no one home i said N what are you selling anyway im not interested i N said well you have a nice day they said heres our N card theres a phone number you can call anytime N any other houses down this road anyone else live N here wed like to talk to them about living in fear END\n",
       "13852                                                                                                                                                                                                                                                                                                                                                                                                                                    START philosophic N in its complex , ovoid emptiness , N a skillful pundit coined it as a sort N of stopgap doorstop for those N quaint equations romans never N dreamt of . in form completely clever N and discrete—a mirror come unsilvered , loose watch face without the works , a hollowed globe from tip to toe N unbroken , it evades the grappling N hooks of mass , tilts the thin rim of no thing , remains embryonic sum , non-cogito . END\n",
       "13853                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            START dear writers , im compiling the first in what i hope is a series of publications im calling artists among artists . the theme for issue is “ faggot dinosaur. ” i hope to hear from you ! thank you and best wishes . END"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6047bbd-4793-4e2f-8253-475883f55e9d",
    "outputId": "3de94969-859f-4fa8-c0df-b7ca0c8b68c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Poem    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e54db8f-fd00-4736-972c-95dbfb990694",
    "outputId": "705122bb-14e2-4c4d-8ff3-2c1787f0206e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13751"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8568E4YRuaQV"
   },
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Saving poems to a file (required for tokenizer training)\n",
    "poem_file = \"poems.txt\"\n",
    "with open(poem_file, \"w\", encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "    file.write('\\n'.join(df['Poem']))\n",
    "\n",
    "# Train the tokenizer on the poems\n",
    "tokenizer.train(files=[poem_file], vocab_size=30000, min_frequency=1, special_tokens=[\"START\", \"END\", \"N\", \"UNK\", \"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0UmIT2dIuaQW",
    "outputId": "e01ddb92-c00e-45f7-cec2-b2442bd59d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 longest tokenized poem lengths: [22321, 14606, 13912, 12385, 11251, 10661, 10216, 9995, 9930, 9822, 9761, 9622, 8799, 8577, 8125]\n",
      "Longest length after operations: 10001\n"
     ]
    }
   ],
   "source": [
    "# Tokenize poems and store the token IDs\n",
    "df['Tokenized_Poem'] = df['Poem'].apply(lambda poem: tokenizer.encode(poem).ids)\n",
    "df['Token_Length'] = df['Tokenized_Poem'].apply(len)\n",
    "\n",
    "# Find the longest lengths and sort them\n",
    "sorted_lengths = sorted(df['Token_Length'], reverse=True)\n",
    "top_15_lengths = sorted_lengths[:15]\n",
    "print(\"Top 15 longest tokenized poem lengths:\", top_15_lengths)\n",
    "\n",
    "max_length = 10000\n",
    "\n",
    "# Ensure END token is known or defined\n",
    "END_token_id = tokenizer.token_to_id(\"END\")\n",
    "\n",
    "# Function to trim poems and append END token\n",
    "def trim_and_append_end(poem_tokens):\n",
    "    if len(poem_tokens) > max_length:\n",
    "        return poem_tokens[:max_length] + [END_token_id]\n",
    "    return poem_tokens\n",
    "\n",
    "# Apply trimming and END token appending\n",
    "df['Tokenized_Poem'] = df['Tokenized_Poem'].apply(trim_and_append_end)\n",
    "\n",
    "# Update the 'Token_Length' to reflect the post-operation lengths\n",
    "df['Token_Length'] = df['Tokenized_Poem'].apply(len)\n",
    "\n",
    "# Appoint longest_length as the longest poem length after all operations\n",
    "longest_length = max(df['Token_Length'])\n",
    "print(\"Longest length after operations:\", longest_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0cb421f7-9dac-4651-a2cb-9167ffad6ab4"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()  # Number of unique words in the vocabulary\n",
    "embedding_dim = 128\n",
    "batch_size = 1 # Increase batch size if resources allow as it bring stabilization, 1 is very noisy\n",
    "learning_rate = 0.01 # changed lr because maybe the embedding dim is too low and lr is too high so gradient is just bouncing around and not learning much\n",
    "pad_index = 1\n",
    "heads = 4\n",
    "epochs = 1\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0c163508-045d-4b2e-b9e2-364eb5ee1344",
    "outputId": "fa6324b9-0579-486b-d759-35be5bd5de54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded inputs tensor: torch.Size([13751, 10001])\n"
     ]
    }
   ],
   "source": [
    "pad_token = tokenizer.token_to_id(\"PAD\")\n",
    "padded_poems = torch.tensor([poem + [pad_token] * (longest_length - len(poem)) for poem in df['Tokenized_Poem']])\n",
    "\n",
    "# Creating PyTorch DataLoader\n",
    "dataset = TensorDataset(padded_poems)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Verify shapes (optional)\n",
    "print(f\"Shape of padded inputs tensor: {padded_poems.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "e2e72b79-940c-4aac-9e26-6f4428469425"
   },
   "outputs": [],
   "source": [
    "# for i in data_loader:\n",
    "#     print(i[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ae6bi9LpuaQW",
    "outputId": "61b8a90d-6550-4171-934e-058f893618cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3cc71826-b2db-4607-96cb-fdb8fe327305"
   },
   "outputs": [],
   "source": [
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "97ec5fef-72c5-4524-b4b2-2c3df5905682"
   },
   "outputs": [],
   "source": [
    "# Now that we have successfully trained our word embedding layer, time to make our positional matrix, which we will make using the formula mentioned in the paper\n",
    "def positional_encoding(seq_len, embedding_dim, device=device):\n",
    "    pe = torch.zeros(seq_len, embedding_dim, device=device)\n",
    "    # Compute the positional encoding values\n",
    "    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n",
    "    # Adjust div_term calculation to handle odd embedding_dim\n",
    "    div_term_exp = torch.arange(0, embedding_dim, 2).float() * -(np.log(10000.0) / embedding_dim)\n",
    "    div_term = torch.exp(div_term_exp).to(device)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    if embedding_dim % 2 == 0:  # Check if embedding_dim is even\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # For even embedding_dim\n",
    "    else:\n",
    "        # Adjust the last cosine computation for odd embedding_dim\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]  # Exclude the last column for odd embedding_dim\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fa380b5-1465-483c-b1ea-562a7978bef1",
    "outputId": "8dca286a-f9db-4556-a763-828ba1503edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10001, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_pos_encoding = positional_encoding(longest_length, embedding_dim)\n",
    "encoder_pos_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "eb6f80c9-fd5a-4014-8750-3724b592e8fd"
   },
   "outputs": [],
   "source": [
    "# Time to make the Multi-head Self Attention block\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Calculate attention\n",
    "        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(embedding_dim))), V)\n",
    "\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "6e0155ef-eee5-41c3-8c1b-48d9f40dc0ee"
   },
   "outputs": [],
   "source": [
    "# Time to make the Masked Multi-head Self Attention block\n",
    "class MaskedMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super(MaskedMultiHeadSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Create a mask for masking the attention score\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).unsqueeze(0).unsqueeze(0).expand(batch_size, self.heads, -1, -1).to(device)\n",
    "        value_to_fill = float('-inf')\n",
    "\n",
    "        # Calculate attention (including mask)\n",
    "        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)).masked_fill(mask, value_to_fill) / torch.sqrt(torch.tensor(embedding_dim))), V)\n",
    "\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[6, 6, 3, 6, 2],\n",
       "          [5, 9, 7, 9, 3],\n",
       "          [8, 8, 5, 7, 2],\n",
       "          [4, 9, 1, 5, 2]],\n",
       "\n",
       "         [[9, 8, 1, 7, 1],\n",
       "          [2, 7, 1, 2, 3],\n",
       "          [8, 2, 8, 1, 1],\n",
       "          [4, 3, 1, 1, 9]],\n",
       "\n",
       "         [[7, 7, 9, 1, 4],\n",
       "          [1, 2, 3, 1, 5],\n",
       "          [5, 6, 8, 4, 3],\n",
       "          [7, 8, 8, 8, 3]]],\n",
       "\n",
       "\n",
       "        [[[8, 6, 6, 6, 3],\n",
       "          [6, 3, 7, 3, 8],\n",
       "          [8, 2, 6, 9, 9],\n",
       "          [2, 4, 8, 1, 3]],\n",
       "\n",
       "         [[9, 4, 1, 6, 8],\n",
       "          [7, 2, 3, 2, 5],\n",
       "          [3, 4, 4, 1, 3],\n",
       "          [8, 8, 5, 3, 7]],\n",
       "\n",
       "         [[4, 2, 3, 7, 7],\n",
       "          [9, 2, 1, 1, 7],\n",
       "          [4, 9, 3, 1, 8],\n",
       "          [2, 3, 5, 6, 5]]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(1, 10, size=(2, 3, 4, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[6, 6, 3, 6, 2, 9, 8, 1, 7, 1, 7, 7, 9, 1, 4],\n",
       "         [5, 9, 7, 9, 3, 2, 7, 1, 2, 3, 1, 2, 3, 1, 5],\n",
       "         [8, 8, 5, 7, 2, 8, 2, 8, 1, 1, 5, 6, 8, 4, 3],\n",
       "         [4, 9, 1, 5, 2, 4, 3, 1, 1, 9, 7, 8, 8, 8, 3]],\n",
       "\n",
       "        [[8, 6, 6, 6, 3, 9, 4, 1, 6, 8, 4, 2, 3, 7, 7],\n",
       "         [6, 3, 7, 3, 8, 7, 2, 3, 2, 5, 9, 2, 1, 1, 7],\n",
       "         [8, 2, 6, 9, 9, 3, 4, 4, 1, 3, 4, 9, 3, 1, 8],\n",
       "         [2, 4, 8, 1, 3, 8, 8, 5, 3, 7, 2, 3, 5, 6, 5]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(1, 2).reshape(2, 4, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "c54ecdfc-9f1c-4120-8578-46e05d1c2069"
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(AddNorm, self).__init__()\n",
    "        # Layer Norm will normalize the last dimension of the matrix\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "\n",
    "    def forward(self, original, modified):\n",
    "        return self.norm(original + modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "a1a5b4a5-96f7-40ba-9193-ab6e0dd7ccfc"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # Normally nn.Linear(embedding_dim, embedding_dim * 4) for expressiveness, we will change it if we have the resources to do so\n",
    "        self.lr1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.lr2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9fe1f5ed-35ce-479b-b596-d2cc95a2c06c"
   },
   "outputs": [],
   "source": [
    "# Time to build the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, batch_size, heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.word_embedder = WordEmbedder(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = positional_encoding(longest_length, embedding_dim)\n",
    "        self.masked_attention = MaskedMultiHeadSelfAttention(heads, embedding_dim)\n",
    "        self.add_norm1 = AddNorm(embedding_dim)\n",
    "        self.attention = MultiHeadSelfAttention(heads, embedding_dim)\n",
    "        self.add_norm2 = AddNorm(embedding_dim)\n",
    "        self.feed_forward = FeedForward(embedding_dim)\n",
    "        self.add_norm3 = AddNorm(embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.word_embedder(x)\n",
    "        x = x + self.positional_encoding[:x.size(1), :].unsqueeze(0).expand(x.size(0), -1, -1)\n",
    "        x = self.add_norm1(x, self.masked_attention(x))\n",
    "        x = self.add_norm2(x, self.attention(x))\n",
    "        x = self.add_norm3(x, self.feed_forward(x))\n",
    "        logits = self.linear(x)\n",
    "        return F.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "881cd445-c7fb-4f2b-ae1f-d6807b2f76a5"
   },
   "outputs": [],
   "source": [
    "model = Decoder(vocab_size, embedding_dim, batch_size, heads).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "3026a026-a1e7-4f12-9f82-8a78982bb1ed"
   },
   "outputs": [],
   "source": [
    "# Initialize our optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbfdc8bd-74e2-45f3-87ea-03c882aa2dee",
    "outputId": "dd835b43-6df6-4fe5-dfbe-1c87dddc1634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 13751/13751 [15:36<00:00, 14.68it/s, loss=10.3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.2361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(data_loader, model, optimizer, loss_fn, device, pad_token):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Wrap your data loader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=True)\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        torch.cuda.empty_cache()\n",
    "        poems = batch[0].to(device)  # Load poems to the device\n",
    "\n",
    "        # Find the max length in this batch to minimize computation\n",
    "        max_length = poems.ne(pad_token).sum(1).max().item()  # Assuming padding token ID is pad_token\n",
    "        poems = poems[:, :max_length]  # Trim to max length\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prepare input and target sequences\n",
    "        input_sequences = poems[:, :-1]  # All tokens except the last\n",
    "        target_sequences = poems[:, 1:]  # All tokens except the first\n",
    "\n",
    "        # Forward pass using teacher forcing\n",
    "        logits = model(input_sequences)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits.transpose(1, 2), target_sequences)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar with the current batch loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return average_loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{epochs}\")\n",
    "    epoch_loss = train_epoch(data_loader, model, optimizer, loss_fn, device, pad_token)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "04YLCbmquaQY"
   },
   "outputs": [],
   "source": [
    "model_path = \"model_weights.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJ30__kSuaQY",
    "outputId": "c6f8f46e-d6bb-4a37-9ea3-0e70e0b6cb1b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (word_embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(30000, 128)\n",
       "  )\n",
       "  (masked_attention): MaskedMultiHeadSelfAttention(\n",
       "    (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (w_a): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (add_norm1): AddNorm(\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (attention): MultiHeadSelfAttention(\n",
       "    (w_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (w_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (w_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (w_a): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (add_norm2): AddNorm(\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (feed_forward): FeedForward(\n",
       "    (lr1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (lr2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (add_norm3): AddNorm(\n",
       "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=128, out_features=30000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "5132ae30-6205-4e12-a54e-b58b3f03e1bf"
   },
   "outputs": [],
   "source": [
    "def generate_poem(model, start_sequence, tokenizer=tokenizer, max_length=100):\n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    tokens = tokenizer.encode(start_sequence)  # Encode the start sequence to tokens directly using the tokenizer.\n",
    "\n",
    "    # Initialize the sequence with the token IDs of the start sequence\n",
    "    generated_ids = tokens.ids\n",
    "\n",
    "    for _ in range(max_length - len(generated_ids)):\n",
    "        input_seq = torch.tensor([generated_ids], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output_logits = model(input_seq)\n",
    "\n",
    "        # Predict the next token ID (not index).\n",
    "        next_token_id = output_logits[:, -1, :].argmax(-1).item()\n",
    "        generated_ids.append(next_token_id)  # Append the ID of the next token.\n",
    "\n",
    "        # Stop if the end token is generated.\n",
    "        end_token_id = tokenizer.token_to_id(\"END\")\n",
    "        if next_token_id == end_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode the token IDs back to text using the tokenizer.\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01b5e8f6-5146-4019-9f45-08c003279b49",
    "outputId": "b75d8021-cc0c-464e-a6e6-740b75d1c445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the starting words for your poem: the ball\n",
      "the ball                                                                                                  \n"
     ]
    }
   ],
   "source": [
    "start_sequence = input(\"Please enter the starting words for your poem: \")\n",
    "poem = generate_poem(model, start_sequence)\n",
    "print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "23f509d4-499c-4267-928b-b0379cde2b36"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 236282,
     "sourceId": 502516,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
