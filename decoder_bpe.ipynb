{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":502516,"sourceType":"datasetVersion","datasetId":236282},{"sourceId":8147125,"sourceType":"datasetVersion","datasetId":4817926}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nimport torch\nimport math\nimport re\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\nimport numpy as np\nfrom tokenizers import ByteLevelBPETokenizer\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport nltk\nnltk.download('punkt')","metadata":{"id":"7e893663-462c-4e19-98f0-2611f454ccf7","outputId":"96f88eb4-3db5-4384-94ad-74718a1b50fb","execution":{"iopub.status.busy":"2024-05-10T13:15:02.379607Z","iopub.execute_input":"2024-05-10T13:15:02.380376Z","iopub.status.idle":"2024-05-10T13:15:02.389411Z","shell.execute_reply.started":"2024-05-10T13:15:02.380344Z","shell.execute_reply":"2024-05-10T13:15:02.388418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/poetry/poetry.csv\")","metadata":{"id":"46a99932-f65a-4ffd-981b-4eb2c1c32775","execution":{"iopub.status.busy":"2024-05-10T13:15:02.393509Z","iopub.execute_input":"2024-05-10T13:15:02.394147Z","iopub.status.idle":"2024-05-10T13:15:02.720517Z","shell.execute_reply.started":"2024-05-10T13:15:02.394121Z","shell.execute_reply":"2024-05-10T13:15:02.719721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.iloc[:len(df)]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:15:02.722330Z","iopub.execute_input":"2024-05-10T13:15:02.722645Z","iopub.status.idle":"2024-05-10T13:15:02.727448Z","shell.execute_reply.started":"2024-05-10T13:15:02.722618Z","shell.execute_reply":"2024-05-10T13:15:02.726450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ndf.tail()","metadata":{"id":"259e65ac-45c6-452d-b220-6f9e3f605093","outputId":"1d03b069-37f4-4e1d-9199-71d98240230c","execution":{"iopub.status.busy":"2024-05-10T13:15:02.728568Z","iopub.execute_input":"2024-05-10T13:15:02.728817Z","iopub.status.idle":"2024-05-10T13:15:02.745199Z","shell.execute_reply.started":"2024-05-10T13:15:02.728796Z","shell.execute_reply":"2024-05-10T13:15:02.744186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"68c8a2b5-553f-494e-bed7-b87dda4a31a0","outputId":"f585a655-b3bb-47bb-cd1a-b220d06c67a9","execution":{"iopub.status.busy":"2024-05-10T13:15:02.747518Z","iopub.execute_input":"2024-05-10T13:15:02.747811Z","iopub.status.idle":"2024-05-10T13:15:02.764897Z","shell.execute_reply.started":"2024-05-10T13:15:02.747787Z","shell.execute_reply":"2024-05-10T13:15:02.763885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['Unnamed: 0', 'Tags', 'Poet', 'Title'], axis=1)","metadata":{"id":"bfb30a34-a0f3-4202-b652-931b29a215a1","execution":{"iopub.status.busy":"2024-05-10T13:15:02.766040Z","iopub.execute_input":"2024-05-10T13:15:02.766415Z","iopub.status.idle":"2024-05-10T13:15:02.772859Z","shell.execute_reply.started":"2024-05-10T13:15:02.766388Z","shell.execute_reply":"2024-05-10T13:15:02.771835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df['Title'] = df['Title'].str.replace('\\r', '')\ndf['Poem'] = df['Poem'].str.replace('\\r', '')\n# df['Title'] = df['Title'].str.replace('\\n', '')\ndf['Poem'] = df['Poem'].str.strip('\\n')","metadata":{"id":"744c8e4b-7444-4c88-bceb-d1986680a5da","execution":{"iopub.status.busy":"2024-05-10T13:15:02.774127Z","iopub.execute_input":"2024-05-10T13:15:02.774459Z","iopub.status.idle":"2024-05-10T13:15:02.861989Z","shell.execute_reply.started":"2024-05-10T13:15:02.774433Z","shell.execute_reply":"2024-05-10T13:15:02.861161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"id":"XHKDr0NhuaQU","outputId":"b58474e3-1194-4563-e6c3-80a0aca43ee4","execution":{"iopub.status.busy":"2024-05-10T13:15:02.863188Z","iopub.execute_input":"2024-05-10T13:15:02.863538Z","iopub.status.idle":"2024-05-10T13:15:02.872101Z","shell.execute_reply.started":"2024-05-10T13:15:02.863507Z","shell.execute_reply":"2024-05-10T13:15:02.871001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lower case everything\n# df['Title'] = df['Title'].str.lower()\ndf['Poem'] = df['Poem'].str.lower()\n\n# Remove apostrophes and join the parts\n# df['Title'] = df['Title'].str.replace(\"'\", \"\", regex=False).str.replace(\"’\", \"\", regex=False)\ndf['Poem'] = df['Poem'].str.replace(\"'\", \"\", regex=False).str.replace(\"’\", \"\", regex=False)","metadata":{"id":"147cacee-0272-4883-a8b0-5e9f9cc42621","execution":{"iopub.status.busy":"2024-05-10T13:15:02.873325Z","iopub.execute_input":"2024-05-10T13:15:02.873720Z","iopub.status.idle":"2024-05-10T13:15:03.110877Z","shell.execute_reply.started":"2024-05-10T13:15:02.873690Z","shell.execute_reply":"2024-05-10T13:15:03.109844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Uncomment only when including encoder, until then let it be commented off\n# Filter out rows where both 'Title' and 'Poem' have no alphabetic characters\n\n# df = df[df.apply(lambda x: any(c.isalpha() for c in x['Title']) and\n#                             any(c.isalpha() for c in x['Poem']), axis=1)]\n\n# Filter out rows with no value (its not NaN its '' in this dataset)\ndf = df[df.apply(lambda x: any(c.isalpha() for c in x['Poem']), axis=1)]","metadata":{"id":"d53c2016-cbe5-4bcf-9a11-94f7abf27a6a","execution":{"iopub.status.busy":"2024-05-10T13:15:03.112069Z","iopub.execute_input":"2024-05-10T13:15:03.112415Z","iopub.status.idle":"2024-05-10T13:15:03.274342Z","shell.execute_reply.started":"2024-05-10T13:15:03.112383Z","shell.execute_reply":"2024-05-10T13:15:03.273521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make newline a special token\n# df['Title'] = 'START ' + df['Title'] + ' END'\ndf['Poem'] = ' START ' + df['Poem'] + ' END '\n\ndf['Poem'] = df['Poem'].apply(lambda x: re.sub(r'\\n', ' N ', x))","metadata":{"id":"54d2c668-0a97-4e00-8976-f5260fb83bd7","outputId":"de90cac7-0b17-45ec-d9ae-46c21a88a93d","execution":{"iopub.status.busy":"2024-05-10T13:15:03.284020Z","iopub.execute_input":"2024-05-10T13:15:03.284301Z","iopub.status.idle":"2024-05-10T13:15:03.415528Z","shell.execute_reply.started":"2024-05-10T13:15:03.284277Z","shell.execute_reply":"2024-05-10T13:15:03.414714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_numeric_tokens(text):\n    tokens = word_tokenize(text)\n    # Filter tokens to exclude any that contain digits\n    filtered_tokens = [token for token in tokens if not re.search(r'\\d', token)]\n    processed_text = ' '.join(filtered_tokens)\n    return processed_text\n\ndf['Poem'] = df['Poem'].apply(remove_numeric_tokens)","metadata":{"id":"255a982a-7b39-49eb-814f-c56e15fc6b44","execution":{"iopub.status.busy":"2024-05-10T13:15:03.416813Z","iopub.execute_input":"2024-05-10T13:15:03.417089Z","iopub.status.idle":"2024-05-10T13:15:51.582434Z","shell.execute_reply.started":"2024-05-10T13:15:03.417065Z","shell.execute_reply":"2024-05-10T13:15:51.581468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"id":"HuEV8lK0uaQV","outputId":"a5c549af-cf15-408e-c308-eb9da816291a","execution":{"iopub.status.busy":"2024-05-10T13:15:51.584061Z","iopub.execute_input":"2024-05-10T13:15:51.584398Z","iopub.status.idle":"2024-05-10T13:15:51.593647Z","shell.execute_reply.started":"2024-05-10T13:15:51.584367Z","shell.execute_reply":"2024-05-10T13:15:51.592803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"b6047bbd-4793-4e2f-8253-475883f55e9d","outputId":"3de94969-859f-4fa8-c0df-b7ca0c8b68c6","execution":{"iopub.status.busy":"2024-05-10T13:15:51.594815Z","iopub.execute_input":"2024-05-10T13:15:51.595090Z","iopub.status.idle":"2024-05-10T13:15:51.606393Z","shell.execute_reply.started":"2024-05-10T13:15:51.595067Z","shell.execute_reply":"2024-05-10T13:15:51.605635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.size","metadata":{"id":"9e54db8f-fd00-4736-972c-95dbfb990694","outputId":"705122bb-14e2-4c4d-8ff3-2c1787f0206e","execution":{"iopub.status.busy":"2024-05-10T13:15:51.607602Z","iopub.execute_input":"2024-05-10T13:15:51.608179Z","iopub.status.idle":"2024-05-10T13:15:51.614956Z","shell.execute_reply.started":"2024-05-10T13:15:51.608147Z","shell.execute_reply":"2024-05-10T13:15:51.614126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = ByteLevelBPETokenizer()\n\n# Saving poems to a file (required for tokenizer training)\npoem_file = \"poems.txt\"\nwith open(poem_file, \"w\", encoding='utf-8') as file: \n    file.write(' N '.join(df['Poem']))\n\n# Train the tokenizer on the poems\ntokenizer.train(files=[poem_file], vocab_size=32000, min_frequency=1, special_tokens=[\"START\", \"END\", \"N\", \"UNK\", \"PAD\"])","metadata":{"id":"8568E4YRuaQV","execution":{"iopub.status.busy":"2024-05-10T13:15:51.616035Z","iopub.execute_input":"2024-05-10T13:15:51.617338Z","iopub.status.idle":"2024-05-10T13:16:05.956875Z","shell.execute_reply.started":"2024-05-10T13:15:51.617307Z","shell.execute_reply":"2024-05-10T13:16:05.955911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = tokenizer.get_vocab_size()  # Number of unique words in the vocabulary\nembedding_dim = 256\nbatch_size = 32 # Increase batch size if resources allow as it bring stabilization, 1 is very noisy\nlearning_rate = 0.001 # changed lr because maybe the embedding dim is too low and lr is too high so gradient is just bouncing around and not learning much\nheads = 4\nepochs = 400\nseq_len = 300\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"0cb421f7-9dac-4651-a2cb-9167ffad6ab4","execution":{"iopub.status.busy":"2024-05-10T13:16:05.958180Z","iopub.execute_input":"2024-05-10T13:16:05.958934Z","iopub.status.idle":"2024-05-10T13:16:05.972774Z","shell.execute_reply.started":"2024-05-10T13:16:05.958898Z","shell.execute_reply":"2024-05-10T13:16:05.971838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(poem_file, \"r\", encoding='utf-8') as file: \n    f = file.read()\n    \ndata = torch.tensor(tokenizer.encode(f).ids)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:05.973949Z","iopub.execute_input":"2024-05-10T13:16:05.974230Z","iopub.status.idle":"2024-05-10T13:16:30.271686Z","shell.execute_reply.started":"2024-05-10T13:16:05.974207Z","shell.execute_reply":"2024-05-10T13:16:30.270877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessing(data, seq_len, pad_token):\n    input_tensor = []\n    target_tensor = []\n    i = 0\n\n    while i < len(data):\n        input_tensor.append(torch.tensor(data[i:i+seq_len]))\n        target_tensor.append(torch.tensor(data[i+1:i+seq_len+1]))\n        i += seq_len\n    \n    input_tensor = torch.nn.utils.rnn.pad_sequence(input_tensor, padding_value=pad_token, batch_first=True)\n    target_tensor = torch.nn.utils.rnn.pad_sequence(target_tensor, padding_value=pad_token, batch_first=True)\n    \n    return input_tensor, target_tensor\n\npad_token = tokenizer.token_to_id(\"PAD\")         \ninput_tensor, target_tensor = preprocessing(data, seq_len, pad_token)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:30.283260Z","iopub.execute_input":"2024-05-10T13:16:30.283615Z","iopub.status.idle":"2024-05-10T13:16:31.251083Z","shell.execute_reply.started":"2024-05-10T13:16:30.283584Z","shell.execute_reply":"2024-05-10T13:16:31.250060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:31.256207Z","iopub.execute_input":"2024-05-10T13:16:31.256524Z","iopub.status.idle":"2024-05-10T13:16:31.263117Z","shell.execute_reply.started":"2024-05-10T13:16:31.256497Z","shell.execute_reply":"2024-05-10T13:16:31.262109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:31.264424Z","iopub.execute_input":"2024-05-10T13:16:31.264815Z","iopub.status.idle":"2024-05-10T13:16:31.274507Z","shell.execute_reply.started":"2024-05-10T13:16:31.264789Z","shell.execute_reply":"2024-05-10T13:16:31.273671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor[51]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:31.275638Z","iopub.execute_input":"2024-05-10T13:16:31.275958Z","iopub.status.idle":"2024-05-10T13:16:31.286629Z","shell.execute_reply.started":"2024-05-10T13:16:31.275933Z","shell.execute_reply":"2024-05-10T13:16:31.285653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_tensor[51]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:31.287828Z","iopub.execute_input":"2024-05-10T13:16:31.288130Z","iopub.status.idle":"2024-05-10T13:16:31.296153Z","shell.execute_reply.started":"2024-05-10T13:16:31.288105Z","shell.execute_reply":"2024-05-10T13:16:31.295164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = TensorDataset(input_tensor, target_tensor)\n\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # Don't shuffle validation data","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:31.298292Z","iopub.execute_input":"2024-05-10T13:16:31.298565Z","iopub.status.idle":"2024-05-10T13:16:31.306007Z","shell.execute_reply.started":"2024-05-10T13:16:31.298534Z","shell.execute_reply":"2024-05-10T13:16:31.305142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_batches(loader, name):\n    n_batches = 0\n    for i in loader:\n        n_batches+=1\n\n    print(f\"Number of batches in {name}: \",n_batches)\n\ncount_batches(train_loader, \"train_loader\")\ncount_batches(val_loader, \"val_loader\")","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:16:31.307044Z","iopub.execute_input":"2024-05-10T13:16:31.307283Z","iopub.status.idle":"2024-05-10T13:16:31.600890Z","shell.execute_reply.started":"2024-05-10T13:16:31.307262Z","shell.execute_reply":"2024-05-10T13:16:31.599959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pad_token","metadata":{"id":"ae6bi9LpuaQW","outputId":"61b8a90d-6550-4171-934e-058f893618cf","execution":{"iopub.status.busy":"2024-05-10T13:16:31.602156Z","iopub.execute_input":"2024-05-10T13:16:31.602441Z","iopub.status.idle":"2024-05-10T13:16:31.607968Z","shell.execute_reply.started":"2024-05-10T13:16:31.602416Z","shell.execute_reply":"2024-05-10T13:16:31.607104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WordEmbedder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n\n    def forward(self, x):\n        return self.embeddings(x)","metadata":{"id":"3cc71826-b2db-4607-96cb-fdb8fe327305","execution":{"iopub.status.busy":"2024-05-10T13:16:31.609036Z","iopub.execute_input":"2024-05-10T13:16:31.609293Z","iopub.status.idle":"2024-05-10T13:16:31.618130Z","shell.execute_reply.started":"2024-05-10T13:16:31.609270Z","shell.execute_reply":"2024-05-10T13:16:31.617369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now that we have successfully trained our word embedding layer, time to make our positional matrix, which we will make using the formula mentioned in the paper\ndef positional_encoding(seq_len, embedding_dim, device=device):\n        positional_encoding = torch.zeros(seq_len, embedding_dim)\n        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n        return positional_encoding","metadata":{"id":"97ec5fef-72c5-4524-b4b2-2c3df5905682","execution":{"iopub.status.busy":"2024-05-10T13:16:31.619200Z","iopub.execute_input":"2024-05-10T13:16:31.619452Z","iopub.status.idle":"2024-05-10T13:16:31.632947Z","shell.execute_reply.started":"2024-05-10T13:16:31.619430Z","shell.execute_reply":"2024-05-10T13:16:31.632154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to make the Multi-head Self Attention block\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, heads, embedding_dim):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.heads = heads\n\n        # 3 Linear Layers for Q, K and V\n        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n\n        # Since the feature or embedding dimension is typically the last dimension\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Last Linear layer for the attention\n        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, embedding_vector):\n        batch_size, seq_len, embedding_dim = embedding_vector.size()\n        # Compute Q, K, and V\n        Q = self.w_q(embedding_vector)\n        K = self.w_k(embedding_vector)\n        V = self.w_v(embedding_vector)\n\n        # Seperate into heads\n        head_dim = embedding_dim // self.heads\n        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n        K = K.view(batch_size, self.heads, seq_len, head_dim)\n        V = V.view(batch_size, self.heads, seq_len, head_dim)\n\n        # Calculate attention\n        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(embedding_dim))), V)\n\n        # Concatenating the attention heads (Transposing for correct concatenation)\n        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n        output = self.w_a(attention)\n        # print(\"Shape after attention:\", output.shape)\n        # num_active_elements = torch.gt(output, -1).sum().item()\n        # total_elements = output.numel()\n        # print(f\"active  att: {num_active_elements}/{total_elements}\")\n        return output","metadata":{"id":"eb6f80c9-fd5a-4014-8750-3724b592e8fd","execution":{"iopub.status.busy":"2024-05-10T13:16:31.638590Z","iopub.execute_input":"2024-05-10T13:16:31.638861Z","iopub.status.idle":"2024-05-10T13:16:31.649923Z","shell.execute_reply.started":"2024-05-10T13:16:31.638838Z","shell.execute_reply":"2024-05-10T13:16:31.648980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to make the Masked Multi-head Self Attention block\nclass MaskedMultiHeadSelfAttention(nn.Module):\n    def __init__(self, heads, embedding_dim):\n        super(MaskedMultiHeadSelfAttention, self).__init__()\n        self.heads = heads\n\n        # 3 Linear Layers for Q, K and V\n        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n\n        # Since the feature or embedding dimension is typically the last dimension\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Last Linear layer for the attention\n        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, embedding_vector):\n        batch_size, seq_len, embedding_dim = embedding_vector.size()\n        # Compute Q, K, and V\n        Q = self.w_q(embedding_vector)\n        K = self.w_k(embedding_vector)\n        V = self.w_v(embedding_vector)\n\n        # Seperate into heads\n        head_dim = embedding_dim // self.heads\n        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n        K = K.view(batch_size, self.heads, seq_len, head_dim)\n        V = V.view(batch_size, self.heads, seq_len, head_dim)\n\n        # Create a mask for masking the attention score\n        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).unsqueeze(0).unsqueeze(0).expand(batch_size, self.heads, -1, -1).to(device)\n        value_to_fill = float('-inf')\n\n        # Calculate attention (including mask)\n        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)).masked_fill(mask, value_to_fill) / torch.sqrt(torch.tensor(embedding_dim))), V)\n        # Concatenating the attention heads (Transposing for correct concatenation)\n        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n        output = self.w_a(attention)\n        # print(\"Shape after mask:\", output.shape)\n        # num_active_elements = torch.gt(output, -1).sum().item()\n        # total_elements = output.numel()\n        # print(f\"active masked: {num_active_elements}/{total_elements}\")\n        return output","metadata":{"id":"6e0155ef-eee5-41c3-8c1b-48d9f40dc0ee","execution":{"iopub.status.busy":"2024-05-10T13:16:31.651327Z","iopub.execute_input":"2024-05-10T13:16:31.652065Z","iopub.status.idle":"2024-05-10T13:16:31.664406Z","shell.execute_reply.started":"2024-05-10T13:16:31.652039Z","shell.execute_reply":"2024-05-10T13:16:31.663537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddNorm(nn.Module):\n    def __init__(self, n_features):\n        super(AddNorm, self).__init__()\n        # Layer Norm will normalize the last dimension of the matrix\n        self.norm = nn.LayerNorm(n_features)\n\n    def forward(self, original, modified):\n        return self.norm(original + modified)","metadata":{"id":"c54ecdfc-9f1c-4120-8578-46e05d1c2069","execution":{"iopub.status.busy":"2024-05-10T13:16:31.665645Z","iopub.execute_input":"2024-05-10T13:16:31.666450Z","iopub.status.idle":"2024-05-10T13:16:31.678203Z","shell.execute_reply.started":"2024-05-10T13:16:31.666418Z","shell.execute_reply":"2024-05-10T13:16:31.677301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, embedding_dim):\n        super(FeedForward, self).__init__()\n        # Normally nn.Linear(embedding_dim, embedding_dim * 4) for expressiveness, we will change it if we have the resources to do so\n        self.lr1 = nn.Linear(embedding_dim, embedding_dim)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, x):\n        x = self.lr1(x)\n        x = self.relu(x)\n        x = self.lr2(x)\n        # num_active_elements = torch.gt(x, -1).sum().item()\n        # total_elements = x.numel()\n        # print(f\"active feedforward: {num_active_elements}/{total_elements}\")\n        return x","metadata":{"id":"a1a5b4a5-96f7-40ba-9193-ab6e0dd7ccfc","execution":{"iopub.status.busy":"2024-05-10T13:16:31.679205Z","iopub.execute_input":"2024-05-10T13:16:31.679439Z","iopub.status.idle":"2024-05-10T13:16:31.688557Z","shell.execute_reply.started":"2024-05-10T13:16:31.679418Z","shell.execute_reply":"2024-05-10T13:16:31.687675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to build the Decoder\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, batch_size, max_length, heads):\n        super(Decoder, self).__init__()\n        self.word_embedder = WordEmbedder(vocab_size, embedding_dim)\n        self.masked_attention = MaskedMultiHeadSelfAttention(heads, embedding_dim)\n        self.add_norm1 = AddNorm(embedding_dim)\n        self.attention = MultiHeadSelfAttention(heads, embedding_dim)\n        self.add_norm2 = AddNorm(embedding_dim)\n        self.feed_forward = FeedForward(embedding_dim)\n        self.add_norm3 = AddNorm(embedding_dim)\n        self.linear = nn.Linear(embedding_dim, vocab_size)\n\n    def forward(self, x):\n        # print(\"Shape before word embedder:\", x.shape)\n        x = self.word_embedder(x)\n        # print(\"Shape after word embedder/before positional:\", x.shape)\n        # num_active_elements = torch.gt(x, -1).sum().item()\n        # total_elements = x.numel()\n        # print(f\"active word embedder: {num_active_elements}/{total_elements}\")\n        x += positional_encoding(x.size(1), embedding_dim).unsqueeze(0).expand(x.size(0), -1, -1).to(device)\n        # print(\"Shape after positional:\", x.shape)\n        # num_active_elements = torch.gt(x, -1).sum().item()\n        # total_elements = x.numel()\n        # print(f\"active positional_encoding: {num_active_elements}/{total_elements}\")\n#         print(x.shape)\n        x = self.add_norm1(x, self.masked_attention(x))\n        # print(\"Shape after addnorm1:\", x.shape)\n        # num_active_elements = torch.gt(x, -1).sum().item()\n        # total_elements = x.numel()\n        # print(f\"active add_norm1: {num_active_elements}/{total_elements}\")\n#         print(x.shape)\n        x = self.add_norm2(x, self.attention(x))\n        # print(\"Shape after addnorm2:\", x.shape)\n        # num_active_elements = torch.gt(x, -1).sum().item()\n        # total_elements = x.numel()\n        # print(f\"active add_nor2: {num_active_elements}/{total_elements}\")\n#         print(x.shape)\n        x = self.add_norm3(x, self.feed_forward(x))\n        # print(\"Shape after addnorm3:\", x.shape)\n        # num_active_elements = torch.gt(x, -1).sum().item()\n        # total_elements = x.numel()\n        # print(f\"active add_norm3: {num_active_elements}/{total_elements}\")\n#         print(x.shape)\n        # print(\"Shape before linear:\", x.shape)\n        logits = self.linear(x)\n        # print(\"Shape after linear:\", logits.shape)\n        # num_active_elements = torch.gt(logits, 0).sum().item()\n        # total_elements = logits.numel()\n        # print(f\"active linear: {num_active_elements}/{total_elements}\")\n        return logits","metadata":{"id":"9fe1f5ed-35ce-479b-b596-d2cc95a2c06c","execution":{"iopub.status.busy":"2024-05-10T13:16:31.689748Z","iopub.execute_input":"2024-05-10T13:16:31.690029Z","iopub.status.idle":"2024-05-10T13:16:31.700798Z","shell.execute_reply.started":"2024-05-10T13:16:31.690006Z","shell.execute_reply":"2024-05-10T13:16:31.699866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Decoder(vocab_size, embedding_dim, batch_size, seq_len, heads).to(device)\nmodel","metadata":{"id":"881cd445-c7fb-4f2b-ae1f-d6807b2f76a5","execution":{"iopub.status.busy":"2024-05-10T13:32:27.363540Z","iopub.execute_input":"2024-05-10T13:32:27.364197Z","iopub.status.idle":"2024-05-10T13:32:27.522843Z","shell.execute_reply.started":"2024-05-10T13:32:27.364166Z","shell.execute_reply":"2024-05-10T13:32:27.521950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize our optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss(ignore_index=pad_token)","metadata":{"id":"3026a026-a1e7-4f12-9f82-8a78982bb1ed","execution":{"iopub.status.busy":"2024-05-10T13:32:28.682757Z","iopub.execute_input":"2024-05-10T13:32:28.683677Z","iopub.status.idle":"2024-05-10T13:32:28.689602Z","shell.execute_reply.started":"2024-05-10T13:32:28.683634Z","shell.execute_reply":"2024-05-10T13:32:28.688699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_poem(model, start_sequence, tokenizer=tokenizer, max_length=500):\n    model.eval()\n    tokens = tokenizer.encode(start_sequence)\n    generated_ids = [tokenizer.token_to_id(\"START\")] + tokens.ids\n    input_seq = torch.tensor([generated_ids], dtype=torch.long).to(device)\n\n    for _ in range(max_length):\n        with torch.no_grad():\n            logits = model(input_seq)\n            logits = logits[:, -1, :]\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            next_token_id = idx_next.item()\n\n            # Append the newly generated token to the input sequence\n            input_seq = torch.cat([input_seq, torch.tensor([next_token_id], dtype=torch.long).unsqueeze(0).to(device)], dim=1)\n            generated_ids.append(next_token_id)\n\n            if next_token_id == tokenizer.token_to_id(\"END\"):\n                break\n\n    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n    return generated_text","metadata":{"id":"5132ae30-6205-4e12-a54e-b58b3f03e1bf","execution":{"iopub.status.busy":"2024-05-10T13:32:30.358231Z","iopub.execute_input":"2024-05-10T13:32:30.358573Z","iopub.status.idle":"2024-05-10T13:32:30.367863Z","shell.execute_reply.started":"2024-05-10T13:32:30.358548Z","shell.execute_reply":"2024-05-10T13:32:30.366703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(data_loader, model, optimizer, loss_fn, device, pad_token):\n  model.train()\n  total_loss = 0.0\n\n  # Wrap your data loader with tqdm for a progress bar\n  progress_bar = tqdm(train_loader, desc=\"Training\", leave=True)\n\n  for input_batch, target_batch in progress_bar:\n    input_data = input_batch.to(device)\n    target_data = target_batch.to(device)\n    optimizer.zero_grad()\n\n    # Forward pass\n    probs = model(input_data)\n\n    # Reshape input and output to correct format for loss calculation\n    B, T, C = probs.shape\n    probs = probs.view(B * T, C)\n    target_data = target_data.view(-1)\n\n    # Calculate loss\n    loss = loss_fn(probs, target_data)\n\n    loss.backward()\n    optimizer.step()\n    total_loss += loss.item()\n\n    # Update the progress bar with the current batch loss\n    progress_bar.set_postfix(loss=loss.item())\n\n  return total_loss / len(data_loader)  # Average loss\n\ndef validate_epoch(data_loader, model, loss_fn, device, pad_token):\n  model.eval()\n  total_loss = 0.0\n\n  with torch.no_grad():\n    # Wrap your data loader with tqdm for a progress bar\n    progress_bar = tqdm(val_loader, desc=\"Validation\", leave=True)\n\n    for input_batch, target_batch in progress_bar:\n      input_data = input_batch.to(device)\n      target_data = target_batch.to(device)\n\n      # Forward pass\n      probs = model(input_data)\n\n      # Reshape input and output to correct format for loss calculation\n      B, T, C = probs.shape\n      probs = probs.view(B * T, C)\n      target_data = target_data.view(-1)\n\n      # Calculate loss\n      loss = loss_fn(probs, target_data)\n\n      total_loss += loss.item()\n\n      # Update the progress bar with the current batch loss (optional)\n      # progress_bar.set_postfix(loss=loss.item())\n\n  return total_loss / len(data_loader)  # Average loss\n\nepoch_losses = []\nvalidation_losses = []\nfor epoch in range(epochs):\n  print(f\"Starting Epoch {epoch + 1}/{epochs}\")\n  train_loss = train_epoch(train_loader, model, optimizer, loss_fn, device, pad_token)\n  val_loss = validate_epoch(val_loader, model, loss_fn, device, pad_token)\n  epoch_losses.append(train_loss)\n  validation_losses.append(val_loss)\n  print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.15f}, Validation Loss: {val_loss:.15f}\")\n  print(generate_poem(model, \"flower\"))\n\n# You can use validation_losses to track model performance and potentially implement early stopping\n","metadata":{"id":"bbfdc8bd-74e2-45f3-87ea-03c882aa2dee","outputId":"dd835b43-6df6-4fe5-dfbe-1c87dddc1634","execution":{"iopub.status.busy":"2024-05-10T13:32:35.674833Z","iopub.execute_input":"2024-05-10T13:32:35.675234Z","iopub.status.idle":"2024-05-10T13:42:37.078496Z","shell.execute_reply.started":"2024-05-10T13:32:35.675203Z","shell.execute_reply":"2024-05-10T13:42:37.077705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(1, epochs + 1), epoch_losses, validation_losses) \nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"Training Loss Over Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:42:37.079874Z","iopub.execute_input":"2024-05-10T13:42:37.080738Z","iopub.status.idle":"2024-05-10T13:42:37.351422Z","shell.execute_reply.started":"2024-05-10T13:42:37.080708Z","shell.execute_reply":"2024-05-10T13:42:37.350598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(validation_losses)","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:45:57.200014Z","iopub.execute_input":"2024-05-10T13:45:57.200377Z","iopub.status.idle":"2024-05-10T13:45:57.206203Z","shell.execute_reply.started":"2024-05-10T13:45:57.200348Z","shell.execute_reply":"2024-05-10T13:45:57.205229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(epochs-5), epoch_losses[5:], label='Training Loss')\nplt.plot(range(epochs-5), validation_losses[5:], label='Validation Loss')\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Training Loss\")\nplt.title(\"Training Loss Over Epochs\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-10T13:48:53.191126Z","iopub.execute_input":"2024-05-10T13:48:53.191860Z","iopub.status.idle":"2024-05-10T13:48:53.414067Z","shell.execute_reply.started":"2024-05-10T13:48:53.191830Z","shell.execute_reply":"2024-05-10T13:48:53.413154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = \"model_weights.pth\"\ntorch.save(model.state_dict(), model_path)","metadata":{"id":"04YLCbmquaQY","execution":{"iopub.status.busy":"2024-05-10T13:26:19.142709Z","iopub.execute_input":"2024-05-10T13:26:19.142981Z","iopub.status.idle":"2024-05-10T13:26:19.242113Z","shell.execute_reply.started":"2024-05-10T13:26:19.142956Z","shell.execute_reply":"2024-05-10T13:26:19.241298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"model_weights.pth\"))\nmodel.eval()","metadata":{"id":"dJ30__kSuaQY","outputId":"c6f8f46e-d6bb-4a37-9ea3-0e70e0b6cb1b","execution":{"iopub.status.busy":"2024-05-10T13:26:19.243216Z","iopub.execute_input":"2024-05-10T13:26:19.243498Z","iopub.status.idle":"2024-05-10T13:26:19.287452Z","shell.execute_reply.started":"2024-05-10T13:26:19.243473Z","shell.execute_reply":"2024-05-10T13:26:19.286488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_sequence = \"flower\"\npoem = generate_poem(model, start_sequence)\nprint(poem)","metadata":{"id":"01b5e8f6-5146-4019-9f45-08c003279b49","outputId":"b75d8021-cc0c-464e-a6e6-740b75d1c445","execution":{"iopub.status.busy":"2024-05-10T13:26:19.320724Z","iopub.execute_input":"2024-05-10T13:26:19.321039Z","iopub.status.idle":"2024-05-10T13:28:05.719011Z","shell.execute_reply.started":"2024-05-10T13:26:19.321012Z","shell.execute_reply":"2024-05-10T13:28:05.718136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"23f509d4-499c-4267-928b-b0379cde2b36"},"execution_count":null,"outputs":[]}]}