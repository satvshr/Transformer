{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7e893663-462c-4e19-98f0-2611f454ccf7",
    "outputId": "96f88eb4-3db5-4384-94ad-74718a1b50fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\satvm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8568E4YRuaQV",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 1115394 \n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# Read the txt file to inspect it\n",
    "with open('tiny-shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(\"Length of dataset:\", len(text), \"\\n\")\n",
    "print(text[:100]) # First 100 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text))) # Get all unique characters in the text\n",
    "vocab_size = len(chars)         # Length of the vocabulary (this includes the space character)\n",
    "print(''.join(chars))\n",
    "print(f'\\nVocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# This is both encoder and decoder\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }     # Character to index mapping\n",
    "itos = { i:ch for i,ch in enumerate(chars) }     # Index to character mapping\n",
    "encode = lambda s: [stoi[c] for c in s]          # Encode a string to a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # Decode a list of integers to a string\n",
    "\n",
    "msg = \"hii there\"\n",
    "token_list = encode(msg)\n",
    "print(token_list)\n",
    "print(decode(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 4178, 612]\n",
      "hii there\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "msg = \"hii there\"\n",
    "token_list = enc.encode(msg)\n",
    "print(token_list) # BPE returns fewer tokens than the character encoding\n",
    "print(enc.decode(enc.encode(\"hii there\")))\n",
    "\n",
    "print(enc.n_vocab) # total amount of tokens in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: torch.Size([1115394]) elements of type torch.int64\n",
      "First 10 tokens from the dataset: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "# Encode the text into a tensor of integers\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'Total size: {data.shape} elements of type {data.dtype}')\n",
    "print('First 10 tokens from the dataset:', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9 * len(data)) # 90% of the data will be used for training and 10% for validation\n",
    "train_data = data[:n]    # 0 to 90-th percentile\n",
    "val_data = data[n:]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 32  # number of sequences in a batch / processed in parallel\n",
    "block_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs shape:  torch.Size([32, 400])\n",
      "tensor([[ 0, 28, 39,  ...,  1, 42, 43],\n",
      "        [53, 59,  1,  ..., 59, 41, 46],\n",
      "        [56, 43, 60,  ...,  1, 44, 47],\n",
      "        ...,\n",
      "        [ 1, 45, 56,  ..., 56, 11,  0],\n",
      "        [44, 43, 50,  ..., 43, 12,  0],\n",
      "        [ 6,  0, 32,  ..., 21, 27, 10]]) \n",
      "\n",
      "targets shape:  torch.Size([32, 400])\n",
      "tensor([[28, 39, 50,  ..., 42, 43, 39],\n",
      "        [59,  1, 57,  ..., 41, 46, 57],\n",
      "        [43, 60, 39,  ..., 44, 47, 56],\n",
      "        ...,\n",
      "        [45, 56, 39,  ..., 11,  0, 32],\n",
      "        [43, 50, 50,  ..., 12,  0,  0],\n",
      "        [ 0, 32, 46,  ..., 27, 10,  0]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split, batch_size):\n",
    "    # Generate a batch of inputs/prompts x and respective targets y\n",
    "    # batches are always of shape (batch_size, block_size)\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Tensor of shape (batch_size,) with random sequence start indices between 0 and len(data) - block_size\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    # Accumulate and add each sequence of this batch to form a tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Same as x but shifted by one token\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y # x is (4,8), y is (4,8) too\n",
    "\n",
    "# Get a batch of inputs and targets\n",
    "xb, yb = get_batch('train', batch_size)\n",
    "\n",
    "# Print the shape of the batch and the actual data\n",
    "print('inputs shape: ', xb.shape)\n",
    "print(xb,'\\n')\n",
    "print('targets shape: ', yb.shape)\n",
    "print(yb, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0cb421f7-9dac-4651-a2cb-9167ffad6ab4"
   },
   "outputs": [],
   "source": [
    "# vocab_size = tokenizer.get_vocab_size()  # Number of unique words in the vocabulary\n",
    "embedding_dim = 256\n",
    "batch_size = 32 # Increase batch size if resources allow as it bring stabilization, 1 is very noisy\n",
    "learning_rate = 0.0001 # changed lr because maybe the embedding dim is too low and lr is too high so gradient is just bouncing around and not learning much\n",
    "heads = 4\n",
    "epochs = 10\n",
    "seq_len = 400\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3cc71826-b2db-4607-96cb-fdb8fe327305"
   },
   "outputs": [],
   "source": [
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "97ec5fef-72c5-4524-b4b2-2c3df5905682"
   },
   "outputs": [],
   "source": [
    "# Now that we have successfully trained our word embedding layer, time to make our positional matrix, which we will make using the formula mentioned in the paper\n",
    "def positional_encoding(seq_len, embedding_dim, device=device):\n",
    "        positional_encoding = torch.zeros(seq_len, embedding_dim)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eb6f80c9-fd5a-4014-8750-3724b592e8fd"
   },
   "outputs": [],
   "source": [
    "# Time to make the Multi-head Self Attention block\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Calculate attention\n",
    "        x= self.softmax((Q @ K.transpose(-1, -2)) / torch.sqrt(torch.tensor(embedding_dim)))\n",
    "        attention = x @ V\n",
    "\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        # print(\"Shape after attention:\", output.shape)\n",
    "        # num_active_elements = torch.gt(output, -1).sum().item()\n",
    "        # total_elements = output.numel()\n",
    "        # print(f\"active  att: {num_active_elements}/{total_elements}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6e0155ef-eee5-41c3-8c1b-48d9f40dc0ee"
   },
   "outputs": [],
   "source": [
    "# Time to make the Masked Multi-head Self Attention block\n",
    "class MaskedMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super(MaskedMultiHeadSelfAttention, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Create a mask for masking the attention score\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).unsqueeze(0).unsqueeze(0).expand(batch_size, self.heads, -1, -1).to(device)\n",
    "        value_to_fill = float('-inf')\n",
    "\n",
    "        # Calculate attention (including mask)\n",
    "        x = self.softmax((Q @  K.transpose(-1, -2)).masked_fill(mask, value_to_fill) / torch.sqrt(torch.tensor(embedding_dim)))\n",
    "        attention = x @ V\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.reshape(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        # print(\"Shape after mask:\", output.shape)\n",
    "        # num_active_elements = torch.gt(output, -1).sum().item()\n",
    "        # total_elements = output.numel()\n",
    "        # print(f\"active masked: {num_active_elements}/{total_elements}\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "c54ecdfc-9f1c-4120-8578-46e05d1c2069"
   },
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(AddNorm, self).__init__()\n",
    "        # Layer Norm will normalize the last dimension of the matrix\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "\n",
    "    def forward(self, original, modified):\n",
    "        return self.norm(original + modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "a1a5b4a5-96f7-40ba-9193-ab6e0dd7ccfc"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # Normally nn.Linear(embedding_dim, embedding_dim * 4) for expressiveness, we will change it if we have the resources to do so\n",
    "        self.lr1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        # num_active_elements = torch.gt(x, -1).sum().item()\n",
    "        # total_elements = x.numel()\n",
    "        # print(f\"active feedforward: {num_active_elements}/{total_elements}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9fe1f5ed-35ce-479b-b596-d2cc95a2c06c"
   },
   "outputs": [],
   "source": [
    "# Time to build the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, batch_size, max_length, heads):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.word_embedder = WordEmbedder(vocab_size, embedding_dim)\n",
    "        self.masked_attention = MaskedMultiHeadSelfAttention(heads, embedding_dim)\n",
    "        self.add_norm1 = AddNorm(embedding_dim)\n",
    "        self.attention = MultiHeadSelfAttention(heads, embedding_dim)\n",
    "        self.add_norm2 = AddNorm(embedding_dim)\n",
    "        self.feed_forward = FeedForward(embedding_dim)\n",
    "        self.add_norm3 = AddNorm(embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Shape before word embedder:\", x.shape)\n",
    "        x = self.word_embedder(x)\n",
    "        # print(\"Shape after word embedder/before positional:\", x.shape)\n",
    "        # num_active_elements = torch.gt(x, -1).sum().item()\n",
    "        # total_elements = x.numel()\n",
    "        # print(f\"active word embedder: {num_active_elements}/{total_elements}\")\n",
    "        x += positional_encoding(x.size(1), embedding_dim).unsqueeze(0).expand(x.size(0), -1, -1).to(device)\n",
    "        # print(\"Shape after positional:\", x.shape)\n",
    "        # num_active_elements = torch.gt(x, -1).sum().item()\n",
    "        # total_elements = x.numel()\n",
    "        # print(f\"active positional_encoding: {num_active_elements}/{total_elements}\")\n",
    "        x = self.add_norm1(x, self.masked_attention(x))\n",
    "        # print(\"Shape after addnorm1:\", x.shape)\n",
    "        # num_active_elements = torch.gt(x, -1).sum().item()\n",
    "        # total_elements = x.numel()\n",
    "        # print(f\"active add_norm1: {num_active_elements}/{total_elements}\")\n",
    "        x = self.add_norm2(x, self.attention(x))\n",
    "        # print(\"Shape after addnorm2:\", x.shape)\n",
    "        # num_active_elements = torch.gt(x, -1).sum().item()\n",
    "        # total_elements = x.numel()\n",
    "        # print(f\"active add_nor2: {num_active_elements}/{total_elements}\")\n",
    "        x = self.add_norm3(x, self.feed_forward(x))\n",
    "        # print(\"Shape after addnorm3:\", x.shape)\n",
    "        # num_active_elements = torch.gt(x, -1).sum().item()\n",
    "        # total_elements = x.numel()\n",
    "        # print(f\"active add_norm3: {num_active_elements}/{total_elements}\")\n",
    "        # print(\"Shape before linear:\", x.shape)\n",
    "        logits = self.linear(x)\n",
    "        # print(\"Shape after linear:\", logits.shape)\n",
    "        # num_active_elements = torch.gt(logits, 0).sum().item()\n",
    "        # total_elements = logits.numel()\n",
    "        # print(f\"active linear: {num_active_elements}/{total_elements}\")\n",
    "        # print(logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "881cd445-c7fb-4f2b-ae1f-d6807b2f76a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (word_embedder): WordEmbedder(\n",
       "    (embeddings): Embedding(65, 256)\n",
       "  )\n",
       "  (masked_attention): MaskedMultiHeadSelfAttention(\n",
       "    (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (w_a): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (add_norm1): AddNorm(\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (attention): MultiHeadSelfAttention(\n",
       "    (w_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (w_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (w_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (softmax): Softmax(dim=-1)\n",
       "    (w_a): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (add_norm2): AddNorm(\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (feed_forward): FeedForward(\n",
       "    (lr1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (lr2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (add_norm3): AddNorm(\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Decoder(65, embedding_dim, batch_size, block_size, heads)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3026a026-a1e7-4f12-9f82-8a78982bb1ed"
   },
   "outputs": [],
   "source": [
    "# Initialize our optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 4.380224704742432\n",
      "Loss at step 10: 3.030172824859619\n",
      "Loss at step 20: 2.7086572647094727\n",
      "Loss at step 30: 2.6185996532440186\n",
      "Loss at step 40: 2.5912744998931885\n",
      "Loss at step 50: 2.531022071838379\n",
      "Loss at step 60: 2.513446092605591\n",
      "Loss at step 70: 2.498901128768921\n",
      "Loss at step 80: 2.475170373916626\n",
      "Loss at step 90: 2.44492769241333\n",
      "Loss at step 100: 2.45695161819458\n",
      "Loss at step 110: 2.4411234855651855\n",
      "Loss at step 120: 2.4021055698394775\n",
      "Loss at step 130: 2.4001097679138184\n",
      "Loss at step 140: 2.397204875946045\n",
      "Loss at step 150: 2.378685712814331\n",
      "Loss at step 160: 2.3240041732788086\n",
      "Loss at step 170: 2.3320224285125732\n",
      "Loss at step 180: 2.3286190032958984\n",
      "Loss at step 190: 2.30329966545105\n",
      "Loss at step 200: 2.2928197383880615\n",
      "Loss at step 210: 2.2707505226135254\n",
      "Loss at step 220: 2.2857577800750732\n",
      "Loss at step 230: 2.2415802478790283\n",
      "Loss at step 240: 2.200723171234131\n",
      "Loss at step 250: 2.1866893768310547\n",
      "Loss at step 260: 2.1699914932250977\n",
      "Loss at step 270: 2.116971731185913\n",
      "Loss at step 280: 2.068103075027466\n",
      "Loss at step 290: 2.129178047180176\n",
      "Loss at step 300: 2.1142334938049316\n",
      "Loss at step 310: 1.988747239112854\n",
      "Loss at step 320: 1.868038535118103\n",
      "Loss at step 330: 1.7571384906768799\n",
      "Loss at step 340: 1.5966256856918335\n",
      "Loss at step 350: 1.4817854166030884\n",
      "Loss at step 360: 1.3110312223434448\n",
      "Loss at step 370: 1.1572576761245728\n",
      "Loss at step 380: 0.9910630583763123\n",
      "Loss at step 390: 0.8616251349449158\n",
      "Loss at step 400: 0.7523226737976074\n",
      "Loss at step 410: 0.6000961065292358\n",
      "Loss at step 420: 0.4969838261604309\n",
      "Loss at step 430: 0.44473275542259216\n",
      "Loss at step 440: 0.3529168665409088\n",
      "Loss at step 450: 0.3283722996711731\n",
      "Loss at step 460: 0.25634148716926575\n",
      "Loss at step 470: 0.21310538053512573\n",
      "Loss at step 480: 0.18223914504051208\n",
      "Loss at step 490: 0.16341058909893036\n",
      "Loss at step 500: 0.13169008493423462\n",
      "Loss at step 510: 0.1171717420220375\n",
      "Loss at step 520: 0.10713072866201401\n",
      "Loss at step 530: 0.08865915238857269\n",
      "Loss at step 540: 0.07693132013082504\n",
      "Loss at step 550: 0.0817175880074501\n",
      "Loss at step 560: 0.07061485946178436\n",
      "Loss at step 570: 0.05272931978106499\n",
      "Loss at step 580: 0.05554672330617905\n",
      "Loss at step 590: 0.050553154200315475\n",
      "Loss at step 600: 0.045804694294929504\n",
      "Loss at step 610: 0.041291333734989166\n",
      "Loss at step 620: 0.03557036817073822\n",
      "Loss at step 630: 0.034204140305519104\n",
      "Loss at step 640: 0.03424978628754616\n",
      "Loss at step 650: 0.030335450544953346\n",
      "Loss at step 660: 0.02818891778588295\n",
      "Loss at step 670: 0.026369305327534676\n",
      "Loss at step 680: 0.02644963003695011\n",
      "Loss at step 690: 0.027459144592285156\n",
      "Loss at step 700: 0.02629430592060089\n",
      "Loss at step 710: 0.021798841655254364\n",
      "Loss at step 720: 0.020306657999753952\n",
      "Loss at step 730: 0.022324394434690475\n",
      "Loss at step 740: 0.022077135741710663\n",
      "Loss at step 750: 0.018634436652064323\n",
      "Loss at step 760: 0.017703186720609665\n",
      "Loss at step 770: 0.02024257369339466\n",
      "Loss at step 780: 0.019688932225108147\n",
      "Loss at step 790: 0.014101325534284115\n",
      "Loss at step 800: 0.019639460369944572\n",
      "Loss at step 810: 0.01546456478536129\n",
      "Loss at step 820: 0.015553352423012257\n",
      "Loss at step 830: 0.01537349820137024\n",
      "Loss at step 840: 0.012341435998678207\n",
      "Loss at step 850: 0.017374496906995773\n",
      "Loss at step 860: 0.01542778592556715\n",
      "Loss at step 870: 0.014292037114501\n",
      "Loss at step 880: 0.01678377576172352\n",
      "Loss at step 890: 0.013668793253600597\n",
      "Loss at step 900: 0.016762692481279373\n",
      "Loss at step 910: 0.013775600120425224\n",
      "Loss at step 920: 0.010625302791595459\n",
      "Loss at step 930: 0.011713209562003613\n",
      "Loss at step 940: 0.011455950327217579\n",
      "Loss at step 950: 0.013702495023608208\n",
      "Loss at step 960: 0.012512299232184887\n",
      "Loss at step 970: 0.012546133249998093\n",
      "Loss at step 980: 0.012724054977297783\n",
      "Loss at step 990: 0.011950012296438217\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # Increasing the batch size from 4 to 32\n",
    "losses = []\n",
    "\n",
    "# Train for 10000 steps/batches\n",
    "for steps in range(1000):\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    optimizer.zero_grad()\n",
    "    probs = model(xb)\n",
    "    # Reshape input and output to correct format for loss calculation\n",
    "    B, T, C = probs.shape\n",
    "    probs = probs.view(B*T, C)\n",
    "    yb = yb.view(-1)\n",
    "    loss = loss_fn(probs, yb)\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    # Print the loss every 100 steps\n",
    "    if steps % 10 == 0:\n",
    "        print(f'Loss at step {steps}: {loss.item()}')\n",
    "        losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), epoch_losses) \n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'epoch_losses' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(range(1, epochs + 1), epoch_losses) \n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04YLCbmquaQY"
   },
   "outputs": [],
   "source": [
    "model_path = \"model_weights.pth\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJ30__kSuaQY",
    "outputId": "c6f8f46e-d6bb-4a37-9ea3-0e70e0b6cb1b"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "5132ae30-6205-4e12-a54e-b58b3f03e1bf"
   },
   "outputs": [],
   "source": [
    "def generate(idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits = model(idx)                              # Forward pass (this is the forward function) with the current sequence of characters idx, results in (B, T, C)\n",
    "        logits = logits[:, -1, :]                          # Focus on the last token from the logits (B, T, C) -> (B, C)\n",
    "        probs = F.softmax(logits, dim=-1)                  # Calculate the probability distribution for the next token based on this last token, results in (B, C)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1) # Sample the next token (B, 1), the token with the highest probability is sampled most likely\n",
    "        idx = torch.cat((idx, idx_next), dim=1)            # Add the new token to the sequence (B, T+1) for the next iteration\n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "23f509d4-499c-4267-928b-b0379cde2b36"
   },
   "outputs": [],
   "source": [
    "# print(decode(generate(torch.rand((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[17]])\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "low = 0\n",
    "high = 66  # High should be 66 to include 65 in the range (exclusive upper bound)\n",
    "\n",
    "# Generate a random integer between low and high (inclusive)\n",
    "random_int = torch.randint(low=low, high=high, size=(1, 1), dtype=torch.long)\n",
    "\n",
    "print(random_int)\n",
    "print(decode(random_int[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEEEE:SEatEatSEEatSS:aItSalS EYc:\n",
      "At EEEEEaaUEEaEtEEEEEELE?EEEELE:EEEEEE:EEEEELaEtcEOEEET:Y:\n",
      "LItrktSpld Gld qupldlplGlGllGlo ETo;ll Tll;\n",
      "ple; hle; fulix! Art JoneF carBe, ETEY;I le. Vor\n",
      "Aqutlle, K:\n",
      "S K:\n",
      "N Y Qu Q Q Que K:\n",
      "H S S K:\n",
      "Noe 'e Y Y H, X QU \n",
      "AEEEEEetc: V'\n",
      "Et K:\n",
      " ENEjeatJK:\n",
      "fo, Eg::hEEf\n",
      "EfeitinifefeifefIfOfefefifeGeGefefifefeBeGerfemifimeGifIEtEfO\n",
      "Ene. \n",
      "Clen! PArdim Joh, Getsu IGem\n",
      "Triht GUCAefeyiaO:\n",
      "Har'OGAO:\n",
      "HO, nHAyFl'lal:\n",
      "Fors, s,-sks, H: Nor:rs, Hirk, k'l'lak:\n",
      "E:'r'H:'r'Ro, b,'s, E:'E\n"
     ]
    }
   ],
   "source": [
    "print(decode(generate(random_int, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 236282,
     "sourceId": 502516,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4817926,
     "sourceId": 8147125,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
