{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e893663-462c-4e19-98f0-2611f454ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a99932-f65a-4ffd-981b-4eb2c1c32775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"poetry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259e65ac-45c6-452d-b220-6f9e3f605093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\r\\n                    Objects Used to Prop Open a Window\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, garlic press\\r\\r\\n     because this window is loose—lacks\\r\\r\\nsuction, lacks grip.\\r\\r\\nBungee cord, bootstrap,\\r\\r\\ndog leash, leather belt\\r\\r\\n     because this window had sash cords.\\r\\r\\nThey frayed. They broke.\\r\\r\\nFeather duster, thatch of straw, empty\\r\\r\\nbottle of Elmer's glue\\r\\r\\n     because this window is loud—its hinges clack\\r\\r\\nopen, clack shut.\\r\\r\\nStuffed bear, baby blanket,\\r\\r\\nsingle crib newel\\r\\r\\n     because this window is split. It's dividing\\r\\r\\nin two.\\r\\r\\nVelvet moss, sagebrush,\\r\\r\\nwillow branch, robin's wing\\r\\r\\n     because this window, it's pane-less. It's only\\r\\r\\na frame of air.\\r\\r\\n</td>\n",
       "      <td>Michelle Menting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\r\\n                    The New Church\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nThe old cupola glinted above the clouds, shone\\r\\r\\namong fir trees, but it took him an hour\\r\\r\\nfor the half mile all the way up the hill. As he trailed,\\r\\r\\nthe village passed him by, greeted him,\\r\\r\\nasked about his health, but everybody hurried\\r\\r\\nto catch the mass, left him leaning against fences,\\r\\r\\nmeasuring the road with the walking stick he sculpted.\\r\\r\\nHe yearned for the day when the new church\\r\\r\\nwould be built—right across the road. Now\\r\\r\\nit rises above the moon: saints in frescoes\\r\\r\\nmeet the eye, and only the rain has started to cut\\r\\r\\nthrough the shingles on the roof of his empty\\r\\r\\nhouse. The apple trees have taken over the sky,\\r\\r\\nsequestered the gate, sidled over the porch.\\r\\r\\n</td>\n",
       "      <td>Lucia Cherciu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\r\\n                    Look for Me\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nLook for me under the hood\\r\\r\\nof that old Chevrolet settled in weeds\\r\\r\\nat the end of the pasture.\\r\\r\\nI'm the radiator that spent its years\\r\\r\\nbolted in front of an engine\\r\\r\\nshoving me forward into the wind.\\r\\r\\nWhatever was in me in those days\\r\\r\\nhas mostly leaked away,\\r\\r\\nbut my cap's still screwed on tight\\r\\r\\nand I know the names of all these\\r\\r\\ntattered moths and broken grasshoppers\\r\\r\\nthe rest of you've forgotten.\\r\\r\\n</td>\n",
       "      <td>Ted Kooser</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\r\\r\\n                    Wild Life\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\nhunches like a giant spider with strange calm:\\r\\r\\nsix tiny babies beneath, each\\r\\r\\nclamoring for a sweet syringe of milk.\\r\\r\\nThis may sound cute to you, reading\\r\\r\\nfrom your pulpit of plenty,\\r\\r\\nbut one small one was left out of reach,\\r\\r\\na knife of fur\\r\\r\\nbarging between the others.\\r\\r\\nI watched behind a turret of sand. If\\r\\r\\nI could have cautioned the mother rabbit\\r\\r\\nI would. If I could summon the\\r\\r\\nBunnies to fit him in beneath\\r\\r\\nthe belly's swell\\r\\r\\nI would. But instead, I stood frozen, wishing\\r\\r\\nfor some equity. This must be\\r\\r\\nwhy it's called Wild Life because of all the\\r\\r\\ncrazed emotions tangled up in\\r\\r\\nthe underbrush within us.\\r\\r\\nDid I tell you how\\r\\r\\nthe smallest one, black and trembling,\\r\\r\\nhopped behind the kudzu\\r\\r\\nstill filigreed with wanting?\\r\\r\\nShould we talk now of animal heritage, their species,\\r\\r\\ncreature development? And what do we say\\r\\r\\nabout form and focus—\\r\\r\\nwriting this when a stray goes hungry, and away.\\r\\r\\n</td>\n",
       "      <td>Grace Cavalieri</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\r\\r\\n                    Umbrella\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nWhen I push your button\\r\\r\\nyou fly off the handle,\\r\\r\\nold skin and bones,\\r\\r\\nblack bat wing.\\r\\r\\nWe're alike, you and I.\\r\\r\\nBoth of us\\r\\r\\nresemble my mother,\\r\\r\\nso fierce in her advocacy\\r\\r\\non behalf of\\r\\r\\nthe most vulnerable child\\r\\r\\nwho'll catch his death\\r\\r\\nin this tempest.\\r\\r\\nSuch a headwind!\\r\\r\\nSometimes it requires\\r\\r\\nall my strength\\r\\r\\njust to end a line.\\r\\r\\nBut when the wind is at\\r\\r\\nmy back, we're likely\\r\\r\\nto get carried away, and say\\r\\r\\nsomething we can never retract,\\r\\r\\nsomething saturated from the ribs\\r\\r\\ndown, an old stony\\r\\r\\nword like ruin. You're what roof\\r\\r\\nI have, frail thing,\\r\\r\\nyou're my argument\\r\\r\\nagainst the whole sky.\\r\\r\\nYou're the fundamental difference\\r\\r\\nbetween wet and dry.\\r\\r\\n</td>\n",
       "      <td>Connie Wanek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                Title  \\\n",
       "0  \\r\\r\\n                    Objects Used to Prop Open a Window\\r\\r\\n                   \n",
       "1                      \\r\\r\\n                    The New Church\\r\\r\\n                   \n",
       "2                         \\r\\r\\n                    Look for Me\\r\\r\\n                   \n",
       "3                           \\r\\r\\n                    Wild Life\\r\\r\\n                   \n",
       "4                            \\r\\r\\n                    Umbrella\\r\\r\\n                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Poem  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                     \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, garlic press\\r\\r\\n     because this window is loose—lacks\\r\\r\\nsuction, lacks grip.\\r\\r\\nBungee cord, bootstrap,\\r\\r\\ndog leash, leather belt\\r\\r\\n     because this window had sash cords.\\r\\r\\nThey frayed. They broke.\\r\\r\\nFeather duster, thatch of straw, empty\\r\\r\\nbottle of Elmer's glue\\r\\r\\n     because this window is loud—its hinges clack\\r\\r\\nopen, clack shut.\\r\\r\\nStuffed bear, baby blanket,\\r\\r\\nsingle crib newel\\r\\r\\n     because this window is split. It's dividing\\r\\r\\nin two.\\r\\r\\nVelvet moss, sagebrush,\\r\\r\\nwillow branch, robin's wing\\r\\r\\n     because this window, it's pane-less. It's only\\r\\r\\na frame of air.\\r\\r\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                 \\r\\r\\nThe old cupola glinted above the clouds, shone\\r\\r\\namong fir trees, but it took him an hour\\r\\r\\nfor the half mile all the way up the hill. As he trailed,\\r\\r\\nthe village passed him by, greeted him,\\r\\r\\nasked about his health, but everybody hurried\\r\\r\\nto catch the mass, left him leaning against fences,\\r\\r\\nmeasuring the road with the walking stick he sculpted.\\r\\r\\nHe yearned for the day when the new church\\r\\r\\nwould be built—right across the road. Now\\r\\r\\nit rises above the moon: saints in frescoes\\r\\r\\nmeet the eye, and only the rain has started to cut\\r\\r\\nthrough the shingles on the roof of his empty\\r\\r\\nhouse. The apple trees have taken over the sky,\\r\\r\\nsequestered the gate, sidled over the porch.\\r\\r\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\r\\r\\nLook for me under the hood\\r\\r\\nof that old Chevrolet settled in weeds\\r\\r\\nat the end of the pasture.\\r\\r\\nI'm the radiator that spent its years\\r\\r\\nbolted in front of an engine\\r\\r\\nshoving me forward into the wind.\\r\\r\\nWhatever was in me in those days\\r\\r\\nhas mostly leaked away,\\r\\r\\nbut my cap's still screwed on tight\\r\\r\\nand I know the names of all these\\r\\r\\ntattered moths and broken grasshoppers\\r\\r\\nthe rest of you've forgotten.\\r\\r\\n   \n",
       "3  \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\nhunches like a giant spider with strange calm:\\r\\r\\nsix tiny babies beneath, each\\r\\r\\nclamoring for a sweet syringe of milk.\\r\\r\\nThis may sound cute to you, reading\\r\\r\\nfrom your pulpit of plenty,\\r\\r\\nbut one small one was left out of reach,\\r\\r\\na knife of fur\\r\\r\\nbarging between the others.\\r\\r\\nI watched behind a turret of sand. If\\r\\r\\nI could have cautioned the mother rabbit\\r\\r\\nI would. If I could summon the\\r\\r\\nBunnies to fit him in beneath\\r\\r\\nthe belly's swell\\r\\r\\nI would. But instead, I stood frozen, wishing\\r\\r\\nfor some equity. This must be\\r\\r\\nwhy it's called Wild Life because of all the\\r\\r\\ncrazed emotions tangled up in\\r\\r\\nthe underbrush within us.\\r\\r\\nDid I tell you how\\r\\r\\nthe smallest one, black and trembling,\\r\\r\\nhopped behind the kudzu\\r\\r\\nstill filigreed with wanting?\\r\\r\\nShould we talk now of animal heritage, their species,\\r\\r\\ncreature development? And what do we say\\r\\r\\nabout form and focus—\\r\\r\\nwriting this when a stray goes hungry, and away.\\r\\r\\n   \n",
       "4                                                                                                                                                                                                                                                                                       \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off the handle,\\r\\r\\nold skin and bones,\\r\\r\\nblack bat wing.\\r\\r\\nWe're alike, you and I.\\r\\r\\nBoth of us\\r\\r\\nresemble my mother,\\r\\r\\nso fierce in her advocacy\\r\\r\\non behalf of\\r\\r\\nthe most vulnerable child\\r\\r\\nwho'll catch his death\\r\\r\\nin this tempest.\\r\\r\\nSuch a headwind!\\r\\r\\nSometimes it requires\\r\\r\\nall my strength\\r\\r\\njust to end a line.\\r\\r\\nBut when the wind is at\\r\\r\\nmy back, we're likely\\r\\r\\nto get carried away, and say\\r\\r\\nsomething we can never retract,\\r\\r\\nsomething saturated from the ribs\\r\\r\\ndown, an old stony\\r\\r\\nword like ruin. You're what roof\\r\\r\\nI have, frail thing,\\r\\r\\nyou're my argument\\r\\r\\nagainst the whole sky.\\r\\r\\nYou're the fundamental difference\\r\\r\\nbetween wet and dry.\\r\\r\\n   \n",
       "\n",
       "               Poet Tags  \n",
       "0  Michelle Menting  NaN  \n",
       "1     Lucia Cherciu  NaN  \n",
       "2        Ted Kooser  NaN  \n",
       "3   Grace Cavalieri  NaN  \n",
       "4      Connie Wanek  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68c8a2b5-553f-494e-bed7-b87dda4a31a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "Title           0\n",
       "Poem            0\n",
       "Poet            0\n",
       "Tags          955\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb30a34-a0f3-4202-b652-931b29a215a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'Tags', 'Poet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744c8e4b-7444-4c88-bceb-d1986680a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].str.replace('\\r', '')\n",
    "df['Poem'] = df['Poem'].str.replace('\\r', '')\n",
    "df['Title'] = df['Title'].str.replace('\\n', '')\n",
    "df['Poem'] = df['Poem'].str.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147cacee-0272-4883-a8b0-5e9f9cc42621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case everything\n",
    "df['Title'] = df['Title'].str.lower()\n",
    "df['Poem'] = df['Poem'].str.lower()\n",
    "\n",
    "# Remove apostrophes and join the parts\n",
    "df['Title'] = df['Title'].str.replace(\"'\", \"\", regex=False)\n",
    "df['Poem'] = df['Poem'].str.replace(\"'\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255a982a-7b39-49eb-814f-c56e15fc6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens containing numbers from the Titles\n",
    "def remove_numeric_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha()]\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "df['Title'] = df['Title'].apply(remove_numeric_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53c2016-cbe5-4bcf-9a11-94f7abf27a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where both 'Title' and 'Poem' have no alphabetic characters\n",
    "df = df[df.apply(lambda x: any(c.isalpha() for c in x['Title']) and \n",
    "                            any(c.isalpha() for c in x['Poem']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14b93fd6-d251-4d70-9bfb-063ee537fa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13829</th>\n",
       "      <td>january</td>\n",
       "      <td>the wise men will unlearn your name.\\nabove your head no star will flame.\\none weary sound will be the same—\\nthe hoarse roar of the gale.\\nthe shadows fall from your tired eyes\\nas your lone bedside candle dies,\\nfor here the calendar breeds nights\\ntill stores of candles fail.\\nwhat prompts this melancholy key?\\na long familiar melody.\\nit sounds again. so let it be.\\nlet it sound from this night.\\nlet it sound in my hour of  death—\\nas gratefulness of eyes and lips\\nfor that which sometimes makes us lift\\nour gaze to the far sky.\\nyou glare in silence at the wall.\\nyour stocking gapes: no gifts at all.\\nits clear that you are now too old\\nto trust in good saint nick;\\nthat its too late for miracles.\\n—but suddenly, lifting your eyes\\nto heavens light, you realize:\\nyour life is a sheer gift.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13831</th>\n",
       "      <td>the bean eaters</td>\n",
       "      <td>they eat beans mostly, this old yellow pair.   \\ndinner is a casual affair.\\nplain chipware on a plain and creaking wood,   \\ntin flatware.\\ntwo who are mostly good.\\ntwo who have lived their day,\\nbut keep on putting on their clothes   \\nand putting things away.\\nand remembering ...\\nremembering, with twinklings and twinges,\\nas they lean over the beans in their rented back room that is full of beads and receipts and dolls and cloths, tobacco crumbs, vases and fringes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13832</th>\n",
       "      <td>the spider</td>\n",
       "      <td>i \\nthe spider expects the cold of winter.\\nwhen the shadows fall in long autumn\\nhe congeals in a nest of paper, prepares\\nthe least and minimal existence,\\nobedient to nature. no other course\\nis his; no other availed him when\\nin high summer he spun and furled\\nthe gaudy catches. i am that spider,\\ncaught in nature, summer and winter.\\nyou are the symbol of the seasons too.\\n \\nii \\nnow to expatiate and temporize\\nthis artful brag. i never saw so quieting\\na sight as the dawn, dew-clenched foot-\\nwide web hung on summer barn-eaves, spangled.\\nit moves to zephyrs that is tough as steel.\\ni never saw so finely-legged a creature\\nwalk so accurate a stretch as he,\\nproud, capable, patient, confident.\\nto the eye he gave close penetration\\ninto real myth, the myth of you, of me.\\n \\niii \\nyet, by moving eyesight off from this\\nthere is another dimension. near the barn,\\ndown meadow to shingle, no place for spiders,\\nthe sea in large blue breathes in brainstorm tides,\\npirates itself away to ancient spain,\\npirouettes past purgatory to paradise.\\ndo i feed deeper on a spider,\\na close-hauled view upon windless meaning,\\nor deeper a day or dance or doom bestride\\non ocean’s long reach, on parables of god?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13833</th>\n",
       "      <td>the distances</td>\n",
       "      <td>the accumulation of reefs\\npiling up one over the others\\nlike thoughts of the sky increasing as the head rises\\nunto horizons of wet december days perforated\\nwith idle motions of gulls . . . and our feelings.\\n \\ni’ve been wondering about what you mean,\\nstanding in the spray of shadows before an ocean\\nabandoned for winter, silent as a barque of blond hair.\\n \\nand the way the clouds are bending, the way they “react”\\nto your position, where your hands close over your breasts\\nlike an eyelid approving the opening of “an evening’s light.”\\n \\nparasites attach themselves to the moss covering\\nyour feet, blind cubans tossing pearls across the jetty,\\nand the sound of blood fixes our eyes on the red waves.\\n \\n                                                                              it is a shark!\\n \\nand our love is that rusted bottle . . . pointing north,\\nthe direction which we turn, conjuring up our silver knives\\nand spoons and erasing messages in the sand, where you wrote\\n \\n“freezing in the arctic of our dreams,” and i said\\n“yes” delaying the cold medium for a time\\nwhile you continued to “cultivate our possessions”\\n \\nas the moon probably “continued” to cradle.\\ntan below the slant of all those wasted trees\\nwhile the scent carried us back to where we were:\\n \\ndancing like the children of great diplomats\\nwith our lean bodies draped in bedsheets and\\nleather flags while the orchestra made sounds\\n \\nwhich we thought was the sky, but was only a series\\nof words, dying in the thick falsetto of mist.\\nfor what can anyone create from all these things:\\n \\nthe fancied tilt of stars, sordid doves\\nburning in the hollow brick oven, oceans\\nwhich generalize tears, it is known to us\\n \\nin immediate gestures, like candle drippings\\non a silk floor. what are we going to do with anything?\\nbesides pick it up gently and lay it on the breath\\n \\nof still another morning, mornings which are\\nalways remaining behind for one thing or another\\nshivering in our faces of pride and blooming attitude.\\n \\nin the draught of winter air my horse is screaming\\nyou are welcoming the new day with your hair leaning\\nagainst the sand, feet dive like otters in the frost\\n \\nand the sudden blue seems to abandon as you leap. o\\nto make everything summer! soldiers move along lines\\nlike wet motions in the violent shade’s reappearance.\\n \\nbut what if your shadow no longer extends to my sleeping?\\nand your youth dissolves in my hand like a tongue, as\\nthe squandered oceans and skies will dissolve into a single plane\\n \\n(so i’ll move along that plane)                unnoticed and gray\\nas a drift of skulls over the cool atlantic where i am\\nstanding now, defining you in perhaps, the only word i can.\\n \\nas other words are appearing, so cunningly, on the lips\\nof the many strips of light. like naked bodies\\nstretched out along the only beach that remained,\\nbrown and perfect below the descending of tides.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>january</td>\n",
       "      <td>the wise men will unlearn your name.\\nabove your head no star will flame.\\none weary sound will be the same—\\nthe hoarse roar of the gale.\\nthe shadows fall from your tired eyes\\nas your lone bedside candle dies,\\nfor here the calendar breeds nights\\ntill stores of candles fail.\\nwhat prompts this melancholy key?\\na long familiar melody.\\nit sounds again. so let it be.\\nlet it sound from this night.\\nlet it sound in my hour of  death—\\nas gratefulness of eyes and lips\\nfor that which sometimes makes us lift\\nour gaze to the far sky.\\nyou glare in silence at the wall.\\nyour stocking gapes: no gifts at all.\\nits clear that you are now too old\\nto trust in good saint nick;\\nthat its too late for miracles.\\n—but suddenly, lifting your eyes\\nto heavens light, you realize:\\nyour life is a sheer gift.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Title  \\\n",
       "13829          january   \n",
       "13831  the bean eaters   \n",
       "13832       the spider   \n",
       "13833    the distances   \n",
       "13848          january   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Poem  \n",
       "13829                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          the wise men will unlearn your name.\\nabove your head no star will flame.\\none weary sound will be the same—\\nthe hoarse roar of the gale.\\nthe shadows fall from your tired eyes\\nas your lone bedside candle dies,\\nfor here the calendar breeds nights\\ntill stores of candles fail.\\nwhat prompts this melancholy key?\\na long familiar melody.\\nit sounds again. so let it be.\\nlet it sound from this night.\\nlet it sound in my hour of  death—\\nas gratefulness of eyes and lips\\nfor that which sometimes makes us lift\\nour gaze to the far sky.\\nyou glare in silence at the wall.\\nyour stocking gapes: no gifts at all.\\nits clear that you are now too old\\nto trust in good saint nick;\\nthat its too late for miracles.\\n—but suddenly, lifting your eyes\\nto heavens light, you realize:\\nyour life is a sheer gift.  \n",
       "13831                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     they eat beans mostly, this old yellow pair.   \\ndinner is a casual affair.\\nplain chipware on a plain and creaking wood,   \\ntin flatware.\\ntwo who are mostly good.\\ntwo who have lived their day,\\nbut keep on putting on their clothes   \\nand putting things away.\\nand remembering ...\\nremembering, with twinklings and twinges,\\nas they lean over the beans in their rented back room that is full of beads and receipts and dolls and cloths, tobacco crumbs, vases and fringes.  \n",
       "13832                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           i \\nthe spider expects the cold of winter.\\nwhen the shadows fall in long autumn\\nhe congeals in a nest of paper, prepares\\nthe least and minimal existence,\\nobedient to nature. no other course\\nis his; no other availed him when\\nin high summer he spun and furled\\nthe gaudy catches. i am that spider,\\ncaught in nature, summer and winter.\\nyou are the symbol of the seasons too.\\n \\nii \\nnow to expatiate and temporize\\nthis artful brag. i never saw so quieting\\na sight as the dawn, dew-clenched foot-\\nwide web hung on summer barn-eaves, spangled.\\nit moves to zephyrs that is tough as steel.\\ni never saw so finely-legged a creature\\nwalk so accurate a stretch as he,\\nproud, capable, patient, confident.\\nto the eye he gave close penetration\\ninto real myth, the myth of you, of me.\\n \\niii \\nyet, by moving eyesight off from this\\nthere is another dimension. near the barn,\\ndown meadow to shingle, no place for spiders,\\nthe sea in large blue breathes in brainstorm tides,\\npirates itself away to ancient spain,\\npirouettes past purgatory to paradise.\\ndo i feed deeper on a spider,\\na close-hauled view upon windless meaning,\\nor deeper a day or dance or doom bestride\\non ocean’s long reach, on parables of god?  \n",
       "13833  the accumulation of reefs\\npiling up one over the others\\nlike thoughts of the sky increasing as the head rises\\nunto horizons of wet december days perforated\\nwith idle motions of gulls . . . and our feelings.\\n \\ni’ve been wondering about what you mean,\\nstanding in the spray of shadows before an ocean\\nabandoned for winter, silent as a barque of blond hair.\\n \\nand the way the clouds are bending, the way they “react”\\nto your position, where your hands close over your breasts\\nlike an eyelid approving the opening of “an evening’s light.”\\n \\nparasites attach themselves to the moss covering\\nyour feet, blind cubans tossing pearls across the jetty,\\nand the sound of blood fixes our eyes on the red waves.\\n \\n                                                                              it is a shark!\\n \\nand our love is that rusted bottle . . . pointing north,\\nthe direction which we turn, conjuring up our silver knives\\nand spoons and erasing messages in the sand, where you wrote\\n \\n“freezing in the arctic of our dreams,” and i said\\n“yes” delaying the cold medium for a time\\nwhile you continued to “cultivate our possessions”\\n \\nas the moon probably “continued” to cradle.\\ntan below the slant of all those wasted trees\\nwhile the scent carried us back to where we were:\\n \\ndancing like the children of great diplomats\\nwith our lean bodies draped in bedsheets and\\nleather flags while the orchestra made sounds\\n \\nwhich we thought was the sky, but was only a series\\nof words, dying in the thick falsetto of mist.\\nfor what can anyone create from all these things:\\n \\nthe fancied tilt of stars, sordid doves\\nburning in the hollow brick oven, oceans\\nwhich generalize tears, it is known to us\\n \\nin immediate gestures, like candle drippings\\non a silk floor. what are we going to do with anything?\\nbesides pick it up gently and lay it on the breath\\n \\nof still another morning, mornings which are\\nalways remaining behind for one thing or another\\nshivering in our faces of pride and blooming attitude.\\n \\nin the draught of winter air my horse is screaming\\nyou are welcoming the new day with your hair leaning\\nagainst the sand, feet dive like otters in the frost\\n \\nand the sudden blue seems to abandon as you leap. o\\nto make everything summer! soldiers move along lines\\nlike wet motions in the violent shade’s reappearance.\\n \\nbut what if your shadow no longer extends to my sleeping?\\nand your youth dissolves in my hand like a tongue, as\\nthe squandered oceans and skies will dissolve into a single plane\\n \\n(so i’ll move along that plane)                unnoticed and gray\\nas a drift of skulls over the cool atlantic where i am\\nstanding now, defining you in perhaps, the only word i can.\\n \\nas other words are appearing, so cunningly, on the lips\\nof the many strips of light. like naked bodies\\nstretched out along the only beach that remained,\\nbrown and perfect below the descending of tides.\\n   \n",
       "13848                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          the wise men will unlearn your name.\\nabove your head no star will flame.\\none weary sound will be the same—\\nthe hoarse roar of the gale.\\nthe shadows fall from your tired eyes\\nas your lone bedside candle dies,\\nfor here the calendar breeds nights\\ntill stores of candles fail.\\nwhat prompts this melancholy key?\\na long familiar melody.\\nit sounds again. so let it be.\\nlet it sound from this night.\\nlet it sound in my hour of  death—\\nas gratefulness of eyes and lips\\nfor that which sometimes makes us lift\\nour gaze to the far sky.\\nyou glare in silence at the wall.\\nyour stocking gapes: no gifts at all.\\nits clear that you are now too old\\nto trust in good saint nick;\\nthat its too late for miracles.\\n—but suddenly, lifting your eyes\\nto heavens light, you realize:\\nyour life is a sheer gift.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d2c668-0a97-4e00-8976-f5260fb83bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make newline a special token\n",
    "df['Poem'] = df['Poem'].str.replace('\\n', ' <N> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6047bbd-4793-4e2f-8253-475883f55e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title    0\n",
       "Poem     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed98f17a-a9cc-4c07-ac3e-73410277b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10911\n"
     ]
    }
   ],
   "source": [
    "tokenized_titles = df['Title'].apply(word_tokenize)\n",
    "# Only consider tokens without numbers in them as  I observed many tokens had numbers in them which to me will just serve as noise\n",
    "all_words = [word for title_words in tokenized_titles for word in title_words if word.isalpha()]\n",
    "unique_words = list(set(all_words))\n",
    "# For unknown tokens(after deployment) and padding\n",
    "special_tokens = [\"<UNK>\", \"<PAD>\"]\n",
    "unique_words = special_tokens + unique_words\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4d92899-2369-495b-8820-4fdd55598d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [objects, used, to, prop, open, a, window]\n",
       "1                                [the, new, church]\n",
       "2                                   [look, for, me]\n",
       "3                                      [wild, life]\n",
       "4                                        [umbrella]\n",
       "                            ...                    \n",
       "13829                                     [january]\n",
       "13831                           [the, bean, eaters]\n",
       "13832                                 [the, spider]\n",
       "13833                              [the, distances]\n",
       "13848                                     [january]\n",
       "Name: Title, Length: 13619, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6992223-63e9-4526-9225-4581f07c5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13619"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Title'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cc71826-b2db-4607-96cb-fdb8fe327305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define hyperparameters (adjust as needed)\n",
    "vocab_size = len(unique_words)  # Number of unique words in the vocabulary\n",
    "embedding_dim = 105  # Number of dimensions for word embeddings\n",
    "context_window_size = 3  # Number of surrounding words considered for context (adjust)\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "device = \"cuda\"\n",
    "# Class for the CBOW-based word embedding model\n",
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "    def forward(self, titles):\n",
    "        batch_size, total_length = titles.shape\n",
    "        pad_index = 1  # Assuming padding index is 1\n",
    "        \n",
    "        # Initialize an empty list to hold the context embeddings\n",
    "        all_context_embeddings = []\n",
    "    \n",
    "        for batch in range(batch_size):\n",
    "            title = titles[batch]\n",
    "    \n",
    "            # Find indices that are not padding\n",
    "            non_pad_indices = (title != pad_index).nonzero().squeeze()\n",
    "            \n",
    "            # Ensure non_pad_indices is a 1D tensor for indexing\n",
    "            if non_pad_indices.dim() == 0:\n",
    "                non_pad_indices = non_pad_indices.unsqueeze(0)  # Make it 1D\n",
    "    \n",
    "            # Calculate start and end indices for slicing the title\n",
    "            start = non_pad_indices[0] - self.context_window_size if non_pad_indices[0] >= self.context_window_size else 0\n",
    "            end = non_pad_indices[-1] + self.context_window_size + 1 if non_pad_indices[-1] + self.context_window_size + 1 <= total_length else total_length\n",
    "            \n",
    "            # Adjusted title with minimal necessary padding\n",
    "            adjusted_title = title[start:end]\n",
    "    \n",
    "            # Now, for each word in the adjusted title, calculate the context embedding\n",
    "            for i in range(self.context_window_size, len(adjusted_title) - self.context_window_size):\n",
    "                context_indices_left = adjusted_title[i-self.context_window_size:i]\n",
    "                context_indices_right = adjusted_title[i+1:i+1+self.context_window_size]\n",
    "                context_indices = torch.cat((context_indices_left, context_indices_right), dim=0).unsqueeze(0)\n",
    "    \n",
    "                # Compute and append the average context embedding\n",
    "                context_embedding = self.embeddings(context_indices).mean(dim=1)\n",
    "                all_context_embeddings.append(context_embedding)\n",
    "    \n",
    "        # Concatenate all context embeddings into a single tensor for processing\n",
    "        all_context_embeddings = torch.cat(all_context_embeddings, dim=0)\n",
    "    \n",
    "        return all_context_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7289b7db-2d6e-495a-b652-04adfa320ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordEmbedder(\n",
      "  (embeddings): Embedding(10911, 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_model = WordEmbedder(vocab_size, embedding_dim, context_window_size).to(device)\n",
    "print(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b85d5c4-2744-4ae2-825b-70abd422108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest title: 35\n",
      "The longest title: within two weeks the african american poet ross gay is mistaken for both the african american poet terrance hayes and the african american poet kyle dargan not one of whom looks anything like the others\n"
     ]
    }
   ],
   "source": [
    "longest_length = 0\n",
    "longest_title = None\n",
    "\n",
    "for title in tokenized_titles:\n",
    "    if len(title) > longest_length:\n",
    "        longest_length = len(title)\n",
    "        longest_title = title\n",
    "\n",
    "print(\"Length of the longest title:\", longest_length)\n",
    "print(\"The longest title:\", ' '.join(longest_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c163508-045d-4b2e-b9e2-364eb5ee1344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We map word to indexes instead of ohe as it takes up lesser space and is faster\n",
    "word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "# Replace all words in the titles with its respective tokens indexes, isb its not in the word_to_index dictionary, replace with 0(index of <UNK>) \n",
    "indexed_titles = [[word_to_index.get(word, word_to_index[\"<UNK>\"]) for word in title] for title in tokenized_titles]\n",
    "\n",
    "# Define the padding value\n",
    "pad_token = word_to_index[\"<PAD>\"]  \n",
    "# Pad the shorter lists with pad_token to match the length of the longest list\n",
    "padded_titles = []\n",
    "for title in indexed_titles:\n",
    "    # We define padding\n",
    "    padding = [pad_token] * context_window_size  \n",
    "    # Padding on both sides\n",
    "    padded_title = padding + title + padding\n",
    "    # Pad until all lists are of equal length\n",
    "    while len(padded_title) < longest_length + (2 * context_window_size): \n",
    "        padded_title.append(pad_token)\n",
    "    padded_titles.append(padded_title)\n",
    "\n",
    "# Convert padded_titles to a PyTorch tensor\n",
    "indexed_titles_tensor = torch.tensor(padded_titles)\n",
    "\n",
    "# Convert input/padded_titles into proper format\n",
    "title_dataset = TensorDataset(indexed_titles_tensor)\n",
    "data_loader = DataLoader(title_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3026a026-a1e7-4f12-9f82-8a78982bb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our optimizer and loss function\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e72b79-940c-4aac-9e26-6f4428469425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 41])\n"
     ]
    }
   ],
   "source": [
    "for i in data_loader:\n",
    "    print(i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99aea03e-a973-4cca-9212-fe73ca4522ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00f7f851-312e-4b53-a13f-857f8cc81313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_with_unk = [title for title in indexed_titles if 0 in title]\n",
    "titles_with_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afe6045c-3b2f-427e-9f64-f673ff42839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Average Loss: 0.7817\n",
      "Epoch [2/15], Average Loss: 0.3672\n",
      "Epoch [3/15], Average Loss: 0.2386\n",
      "Epoch [4/15], Average Loss: 0.1677\n",
      "Epoch [5/15], Average Loss: 0.1222\n",
      "Epoch [6/15], Average Loss: 0.0908\n",
      "Epoch [7/15], Average Loss: 0.0685\n",
      "Epoch [8/15], Average Loss: 0.0522\n",
      "Epoch [9/15], Average Loss: 0.0399\n",
      "Epoch [10/15], Average Loss: 0.0307\n",
      "Epoch [11/15], Average Loss: 0.0237\n",
      "Epoch [12/15], Average Loss: 0.0182\n",
      "Epoch [13/15], Average Loss: 0.0141\n",
      "Epoch [14/15], Average Loss: 0.0109\n",
      "Epoch [15/15], Average Loss: 0.0084\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15  # Set the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader:  # Assuming batch is a tensor of titles\n",
    "        titles = batch[0].to(device)  # Extract titles if DataLoader returns a tuple\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        # Forward pass: compute predicted context embeddings for all titles in the batch\n",
    "        predicted_context_embeddings = embedding_model(titles)\n",
    "        \n",
    "        # A simplified assumption: every non-pad token in `titles` is a target for which\n",
    "        # the model has produced a context embedding. This may need adjusting based on the model.\n",
    "        non_pad_mask = titles != pad_token\n",
    "        target_embeddings = embedding_model.embeddings(titles[non_pad_mask]).to(device)\n",
    "        loss = loss_fn(predicted_context_embeddings, target_embeddings)\n",
    "\n",
    "        # Backward pass: compute gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97ec5fef-72c5-4524-b4b2-2c3df5905682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have successfully trained our word embedding layer, time to make our positional matrix, which we will make using the formula mentioned in the paper\n",
    "def positional_encoding(batch_size, max_seq_length, d_model):\n",
    "    # Initialize the positional encoding matrix\n",
    "    pe = torch.zeros(batch_size, max_seq_length, d_model)\n",
    "    \n",
    "    # Compute the positional encoding values\n",
    "    position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(0).unsqueeze(-1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    # Compute sine and cosine components\n",
    "    pe[:, :, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, :, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fa380b5-1465-483c-b1ea-562a7978bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 41, 100])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(batch_size, longest_length + (2 * context_window_size), embedding_dim)\n",
    "pos_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eb6f80c9-fd5a-4014-8750-3724b592e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to make the Multi-head Self Attention block!\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Calculate attention\n",
    "        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(embedding_dim)), V)\n",
    "\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.transpose(1, 2).view(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5f824cb0-0f9f-4074-9af7-7c1588a229e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5948, 0.4629, 0.9841, 0.9810],\n",
       "          [0.7569, 0.4444, 0.8599, 0.1573]],\n",
       "\n",
       "         [[0.0410, 0.2651, 0.7164, 0.1736],\n",
       "          [0.5461, 0.4083, 0.2160, 0.2952]],\n",
       "\n",
       "         [[0.2096, 0.1985, 0.7276, 0.8832],\n",
       "          [0.0461, 0.4797, 0.6174, 0.1708]]],\n",
       "\n",
       "\n",
       "        [[[0.3996, 0.4926, 0.4556, 0.4357],\n",
       "          [0.0740, 0.8970, 0.2265, 0.8611]],\n",
       "\n",
       "         [[0.1757, 0.5160, 0.1248, 0.7196],\n",
       "          [0.0583, 0.8802, 0.3027, 0.4638]],\n",
       "\n",
       "         [[0.1885, 0.5968, 0.4710, 0.7351],\n",
       "          [0.0434, 0.9290, 0.6689, 0.9437]]]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.rand(2, 3, 2, 4)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e78e421c-e1f7-44d2-b6bc-43c2ec44a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5948, 0.4629, 0.9841, 0.9810, 0.0410, 0.2651, 0.7164, 0.1736,\n",
       "          0.2096, 0.1985, 0.7276, 0.8832],\n",
       "         [0.7569, 0.4444, 0.8599, 0.1573, 0.5461, 0.4083, 0.2160, 0.2952,\n",
       "          0.0461, 0.4797, 0.6174, 0.1708]],\n",
       "\n",
       "        [[0.3996, 0.4926, 0.4556, 0.4357, 0.1757, 0.5160, 0.1248, 0.7196,\n",
       "          0.1885, 0.5968, 0.4710, 0.7351],\n",
       "         [0.0740, 0.8970, 0.2265, 0.8611, 0.0583, 0.8802, 0.3027, 0.4638,\n",
       "          0.0434, 0.9290, 0.6689, 0.9437]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.transpose(1, 2).reshape(2, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fafbe09b-715d-4eb8-93dc-fb35e4730492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5948, 0.4629, 0.9841, 0.9810, 0.7569, 0.4444, 0.8599, 0.1573,\n",
       "          0.0410, 0.2651, 0.7164, 0.1736],\n",
       "         [0.5461, 0.4083, 0.2160, 0.2952, 0.2096, 0.1985, 0.7276, 0.8832,\n",
       "          0.0461, 0.4797, 0.6174, 0.1708]],\n",
       "\n",
       "        [[0.3996, 0.4926, 0.4556, 0.4357, 0.0740, 0.8970, 0.2265, 0.8611,\n",
       "          0.1757, 0.5160, 0.1248, 0.7196],\n",
       "         [0.0583, 0.8802, 0.3027, 0.4638, 0.1885, 0.5968, 0.4710, 0.7351,\n",
       "          0.0434, 0.9290, 0.6689, 0.9437]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.view(2, 2, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ecdfc-9f1c-4120-8578-46e05d1c2069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
