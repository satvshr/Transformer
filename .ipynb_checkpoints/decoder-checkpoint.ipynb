{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e893663-462c-4e19-98f0-2611f454ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a99932-f65a-4ffd-981b-4eb2c1c32775",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"poetry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259e65ac-45c6-452d-b220-6f9e3f605093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\r\\n                    Objects Used to Prop Open a Window\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, garlic press\\r\\r\\n     because this window is loose—lacks\\r\\r\\nsuction, lacks grip.\\r\\r\\nBungee cord, bootstrap,\\r\\r\\ndog leash, leather belt\\r\\r\\n     because this window had sash cords.\\r\\r\\nThey frayed. They broke.\\r\\r\\nFeather duster, thatch of straw, empty\\r\\r\\nbottle of Elmer's glue\\r\\r\\n     because this window is loud—its hinges clack\\r\\r\\nopen, clack shut.\\r\\r\\nStuffed bear, baby blanket,\\r\\r\\nsingle crib newel\\r\\r\\n     because this window is split. It's dividing\\r\\r\\nin two.\\r\\r\\nVelvet moss, sagebrush,\\r\\r\\nwillow branch, robin's wing\\r\\r\\n     because this window, it's pane-less. It's only\\r\\r\\na frame of air.\\r\\r\\n</td>\n",
       "      <td>Michelle Menting</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\r\\n                    The New Church\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nThe old cupola glinted above the clouds, shone\\r\\r\\namong fir trees, but it took him an hour\\r\\r\\nfor the half mile all the way up the hill. As he trailed,\\r\\r\\nthe village passed him by, greeted him,\\r\\r\\nasked about his health, but everybody hurried\\r\\r\\nto catch the mass, left him leaning against fences,\\r\\r\\nmeasuring the road with the walking stick he sculpted.\\r\\r\\nHe yearned for the day when the new church\\r\\r\\nwould be built—right across the road. Now\\r\\r\\nit rises above the moon: saints in frescoes\\r\\r\\nmeet the eye, and only the rain has started to cut\\r\\r\\nthrough the shingles on the roof of his empty\\r\\r\\nhouse. The apple trees have taken over the sky,\\r\\r\\nsequestered the gate, sidled over the porch.\\r\\r\\n</td>\n",
       "      <td>Lucia Cherciu</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\\r\\r\\n                    Look for Me\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nLook for me under the hood\\r\\r\\nof that old Chevrolet settled in weeds\\r\\r\\nat the end of the pasture.\\r\\r\\nI'm the radiator that spent its years\\r\\r\\nbolted in front of an engine\\r\\r\\nshoving me forward into the wind.\\r\\r\\nWhatever was in me in those days\\r\\r\\nhas mostly leaked away,\\r\\r\\nbut my cap's still screwed on tight\\r\\r\\nand I know the names of all these\\r\\r\\ntattered moths and broken grasshoppers\\r\\r\\nthe rest of you've forgotten.\\r\\r\\n</td>\n",
       "      <td>Ted Kooser</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\\r\\r\\n                    Wild Life\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\nhunches like a giant spider with strange calm:\\r\\r\\nsix tiny babies beneath, each\\r\\r\\nclamoring for a sweet syringe of milk.\\r\\r\\nThis may sound cute to you, reading\\r\\r\\nfrom your pulpit of plenty,\\r\\r\\nbut one small one was left out of reach,\\r\\r\\na knife of fur\\r\\r\\nbarging between the others.\\r\\r\\nI watched behind a turret of sand. If\\r\\r\\nI could have cautioned the mother rabbit\\r\\r\\nI would. If I could summon the\\r\\r\\nBunnies to fit him in beneath\\r\\r\\nthe belly's swell\\r\\r\\nI would. But instead, I stood frozen, wishing\\r\\r\\nfor some equity. This must be\\r\\r\\nwhy it's called Wild Life because of all the\\r\\r\\ncrazed emotions tangled up in\\r\\r\\nthe underbrush within us.\\r\\r\\nDid I tell you how\\r\\r\\nthe smallest one, black and trembling,\\r\\r\\nhopped behind the kudzu\\r\\r\\nstill filigreed with wanting?\\r\\r\\nShould we talk now of animal heritage, their species,\\r\\r\\ncreature development? And what do we say\\r\\r\\nabout form and focus—\\r\\r\\nwriting this when a stray goes hungry, and away.\\r\\r\\n</td>\n",
       "      <td>Grace Cavalieri</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>\\r\\r\\n                    Umbrella\\r\\r\\n</td>\n",
       "      <td>\\r\\r\\nWhen I push your button\\r\\r\\nyou fly off the handle,\\r\\r\\nold skin and bones,\\r\\r\\nblack bat wing.\\r\\r\\nWe're alike, you and I.\\r\\r\\nBoth of us\\r\\r\\nresemble my mother,\\r\\r\\nso fierce in her advocacy\\r\\r\\non behalf of\\r\\r\\nthe most vulnerable child\\r\\r\\nwho'll catch his death\\r\\r\\nin this tempest.\\r\\r\\nSuch a headwind!\\r\\r\\nSometimes it requires\\r\\r\\nall my strength\\r\\r\\njust to end a line.\\r\\r\\nBut when the wind is at\\r\\r\\nmy back, we're likely\\r\\r\\nto get carried away, and say\\r\\r\\nsomething we can never retract,\\r\\r\\nsomething saturated from the ribs\\r\\r\\ndown, an old stony\\r\\r\\nword like ruin. You're what roof\\r\\r\\nI have, frail thing,\\r\\r\\nyou're my argument\\r\\r\\nagainst the whole sky.\\r\\r\\nYou're the fundamental difference\\r\\r\\nbetween wet and dry.\\r\\r\\n</td>\n",
       "      <td>Connie Wanek</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                                                Title  \\\n",
       "0  \\r\\r\\n                    Objects Used to Prop Open a Window\\r\\r\\n                   \n",
       "1                      \\r\\r\\n                    The New Church\\r\\r\\n                   \n",
       "2                         \\r\\r\\n                    Look for Me\\r\\r\\n                   \n",
       "3                           \\r\\r\\n                    Wild Life\\r\\r\\n                   \n",
       "4                            \\r\\r\\n                    Umbrella\\r\\r\\n                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Poem  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                     \\r\\r\\nDog bone, stapler,\\r\\r\\ncribbage board, garlic press\\r\\r\\n     because this window is loose—lacks\\r\\r\\nsuction, lacks grip.\\r\\r\\nBungee cord, bootstrap,\\r\\r\\ndog leash, leather belt\\r\\r\\n     because this window had sash cords.\\r\\r\\nThey frayed. They broke.\\r\\r\\nFeather duster, thatch of straw, empty\\r\\r\\nbottle of Elmer's glue\\r\\r\\n     because this window is loud—its hinges clack\\r\\r\\nopen, clack shut.\\r\\r\\nStuffed bear, baby blanket,\\r\\r\\nsingle crib newel\\r\\r\\n     because this window is split. It's dividing\\r\\r\\nin two.\\r\\r\\nVelvet moss, sagebrush,\\r\\r\\nwillow branch, robin's wing\\r\\r\\n     because this window, it's pane-less. It's only\\r\\r\\na frame of air.\\r\\r\\n   \n",
       "1                                                                                                                                                                                                                                                                                                                                 \\r\\r\\nThe old cupola glinted above the clouds, shone\\r\\r\\namong fir trees, but it took him an hour\\r\\r\\nfor the half mile all the way up the hill. As he trailed,\\r\\r\\nthe village passed him by, greeted him,\\r\\r\\nasked about his health, but everybody hurried\\r\\r\\nto catch the mass, left him leaning against fences,\\r\\r\\nmeasuring the road with the walking stick he sculpted.\\r\\r\\nHe yearned for the day when the new church\\r\\r\\nwould be built—right across the road. Now\\r\\r\\nit rises above the moon: saints in frescoes\\r\\r\\nmeet the eye, and only the rain has started to cut\\r\\r\\nthrough the shingles on the roof of his empty\\r\\r\\nhouse. The apple trees have taken over the sky,\\r\\r\\nsequestered the gate, sidled over the porch.\\r\\r\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \\r\\r\\nLook for me under the hood\\r\\r\\nof that old Chevrolet settled in weeds\\r\\r\\nat the end of the pasture.\\r\\r\\nI'm the radiator that spent its years\\r\\r\\nbolted in front of an engine\\r\\r\\nshoving me forward into the wind.\\r\\r\\nWhatever was in me in those days\\r\\r\\nhas mostly leaked away,\\r\\r\\nbut my cap's still screwed on tight\\r\\r\\nand I know the names of all these\\r\\r\\ntattered moths and broken grasshoppers\\r\\r\\nthe rest of you've forgotten.\\r\\r\\n   \n",
       "3  \\r\\r\\nBehind the silo, the Mother Rabbit\\r\\r\\nhunches like a giant spider with strange calm:\\r\\r\\nsix tiny babies beneath, each\\r\\r\\nclamoring for a sweet syringe of milk.\\r\\r\\nThis may sound cute to you, reading\\r\\r\\nfrom your pulpit of plenty,\\r\\r\\nbut one small one was left out of reach,\\r\\r\\na knife of fur\\r\\r\\nbarging between the others.\\r\\r\\nI watched behind a turret of sand. If\\r\\r\\nI could have cautioned the mother rabbit\\r\\r\\nI would. If I could summon the\\r\\r\\nBunnies to fit him in beneath\\r\\r\\nthe belly's swell\\r\\r\\nI would. But instead, I stood frozen, wishing\\r\\r\\nfor some equity. This must be\\r\\r\\nwhy it's called Wild Life because of all the\\r\\r\\ncrazed emotions tangled up in\\r\\r\\nthe underbrush within us.\\r\\r\\nDid I tell you how\\r\\r\\nthe smallest one, black and trembling,\\r\\r\\nhopped behind the kudzu\\r\\r\\nstill filigreed with wanting?\\r\\r\\nShould we talk now of animal heritage, their species,\\r\\r\\ncreature development? And what do we say\\r\\r\\nabout form and focus—\\r\\r\\nwriting this when a stray goes hungry, and away.\\r\\r\\n   \n",
       "4                                                                                                                                                                                                                                                                                       \\r\\r\\nWhen I push your button\\r\\r\\nyou fly off the handle,\\r\\r\\nold skin and bones,\\r\\r\\nblack bat wing.\\r\\r\\nWe're alike, you and I.\\r\\r\\nBoth of us\\r\\r\\nresemble my mother,\\r\\r\\nso fierce in her advocacy\\r\\r\\non behalf of\\r\\r\\nthe most vulnerable child\\r\\r\\nwho'll catch his death\\r\\r\\nin this tempest.\\r\\r\\nSuch a headwind!\\r\\r\\nSometimes it requires\\r\\r\\nall my strength\\r\\r\\njust to end a line.\\r\\r\\nBut when the wind is at\\r\\r\\nmy back, we're likely\\r\\r\\nto get carried away, and say\\r\\r\\nsomething we can never retract,\\r\\r\\nsomething saturated from the ribs\\r\\r\\ndown, an old stony\\r\\r\\nword like ruin. You're what roof\\r\\r\\nI have, frail thing,\\r\\r\\nyou're my argument\\r\\r\\nagainst the whole sky.\\r\\r\\nYou're the fundamental difference\\r\\r\\nbetween wet and dry.\\r\\r\\n   \n",
       "\n",
       "               Poet Tags  \n",
       "0  Michelle Menting  NaN  \n",
       "1     Lucia Cherciu  NaN  \n",
       "2        Ted Kooser  NaN  \n",
       "3   Grace Cavalieri  NaN  \n",
       "4      Connie Wanek  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68c8a2b5-553f-494e-bed7-b87dda4a31a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "Title           0\n",
       "Poem            0\n",
       "Poet            0\n",
       "Tags          955\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb30a34-a0f3-4202-b652-931b29a215a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'Tags', 'Poet'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "744c8e4b-7444-4c88-bceb-d1986680a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Title'] = df['Title'].str.replace('\\r', '')\n",
    "df['Poem'] = df['Poem'].str.replace('\\r', '')\n",
    "df['Title'] = df['Title'].str.replace('\\n', '')\n",
    "df['Poem'] = df['Poem'].str.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147cacee-0272-4883-a8b0-5e9f9cc42621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case everything\n",
    "df['Title'] = df['Title'].str.lower()\n",
    "df['Poem'] = df['Poem'].str.lower()\n",
    "\n",
    "# Remove apostrophes and join the parts\n",
    "df['Title'] = df['Title'].str.replace(\"'\", \"\", regex=False)\n",
    "df['Poem'] = df['Poem'].str.replace(\"'\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255a982a-7b39-49eb-814f-c56e15fc6b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tokens containing numbers from the Titles\n",
    "def remove_numeric_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.isalpha()]\n",
    "    processed_text = ' '.join(filtered_tokens)\n",
    "    return processed_text\n",
    "\n",
    "df['Title'] = df['Title'].apply(remove_numeric_tokens)\n",
    "df['Poem'] = df['Poem'].apply(remove_numeric_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d53c2016-cbe5-4bcf-9a11-94f7abf27a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where both 'Title' and 'Poem' have no alphabetic characters\n",
    "df = df[df.apply(lambda x: any(c.isalpha() for c in x['Title']) and \n",
    "                            any(c.isalpha() for c in x['Poem']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d2c668-0a97-4e00-8976-f5260fb83bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make newline a special token\n",
    "df['Title'] = 'START ' + df['Title'] + ' END'\n",
    "df['Poem'] = 'START ' + df['Poem'] + ' END'\n",
    "\n",
    "df['Poem'] = df['Poem'].str.replace('\\n', ' N ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b93fd6-d251-4d70-9bfb-063ee537fa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13829</th>\n",
       "      <td>START january END</td>\n",
       "      <td>START the wise men will unlearn your name above your head no star will flame one weary sound will be the the hoarse roar of the gale the shadows fall from your tired eyes as your lone bedside candle dies for here the calendar breeds nights till stores of candles fail what prompts this melancholy key a long familiar melody it sounds again so let it be let it sound from this night let it sound in my hour of as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky you glare in silence at the wall your stocking gapes no gifts at all its clear that you are now too old to trust in good saint nick that its too late for miracles suddenly lifting your eyes to heavens light you realize your life is a sheer gift END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13831</th>\n",
       "      <td>START the bean eaters END</td>\n",
       "      <td>START they eat beans mostly this old yellow pair dinner is a casual affair plain chipware on a plain and creaking wood tin flatware two who are mostly good two who have lived their day but keep on putting on their clothes and putting things away and remembering remembering with twinklings and twinges as they lean over the beans in their rented back room that is full of beads and receipts and dolls and cloths tobacco crumbs vases and fringes END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13832</th>\n",
       "      <td>START the spider END</td>\n",
       "      <td>START i the spider expects the cold of winter when the shadows fall in long autumn he congeals in a nest of paper prepares the least and minimal existence obedient to nature no other course is his no other availed him when in high summer he spun and furled the gaudy catches i am that spider caught in nature summer and winter you are the symbol of the seasons too ii now to expatiate and temporize this artful brag i never saw so quieting a sight as the dawn wide web hung on summer spangled it moves to zephyrs that is tough as steel i never saw so a creature walk so accurate a stretch as he proud capable patient confident to the eye he gave close penetration into real myth the myth of you of me iii yet by moving eyesight off from this there is another dimension near the barn down meadow to shingle no place for spiders the sea in large blue breathes in brainstorm tides pirates itself away to ancient spain pirouettes past purgatory to paradise do i feed deeper on a spider a view upon windless meaning or deeper a day or dance or doom bestride on ocean s long reach on parables of god END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13833</th>\n",
       "      <td>START the distances END</td>\n",
       "      <td>START the accumulation of reefs piling up one over the others like thoughts of the sky increasing as the head rises unto horizons of wet december days perforated with idle motions of gulls and our feelings i ve been wondering about what you mean standing in the spray of shadows before an ocean abandoned for winter silent as a barque of blond hair and the way the clouds are bending the way they react to your position where your hands close over your breasts like an eyelid approving the opening of an evening s parasites attach themselves to the moss covering your feet blind cubans tossing pearls across the jetty and the sound of blood fixes our eyes on the red waves it is a shark and our love is that rusted bottle pointing north the direction which we turn conjuring up our silver knives and spoons and erasing messages in the sand where you wrote freezing in the arctic of our dreams and i said yes delaying the cold medium for a time while you continued to cultivate our possessions as the moon probably continued to cradle tan below the slant of all those wasted trees while the scent carried us back to where we were dancing like the children of great diplomats with our lean bodies draped in bedsheets and leather flags while the orchestra made sounds which we thought was the sky but was only a series of words dying in the thick falsetto of mist for what can anyone create from all these things the fancied tilt of stars sordid doves burning in the hollow brick oven oceans which generalize tears it is known to us in immediate gestures like candle drippings on a silk floor what are we going to do with anything besides pick it up gently and lay it on the breath of still another morning mornings which are always remaining behind for one thing or another shivering in our faces of pride and blooming attitude in the draught of winter air my horse is screaming you are welcoming the new day with your hair leaning against the sand feet dive like otters in the frost and the sudden blue seems to abandon as you leap o to make everything summer soldiers move along lines like wet motions in the violent shade s reappearance but what if your shadow no longer extends to my sleeping and your youth dissolves in my hand like a tongue as the squandered oceans and skies will dissolve into a single plane so i ll move along that plane unnoticed and gray as a drift of skulls over the cool atlantic where i am standing now defining you in perhaps the only word i can as other words are appearing so cunningly on the lips of the many strips of light like naked bodies stretched out along the only beach that remained brown and perfect below the descending of tides END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>START january END</td>\n",
       "      <td>START the wise men will unlearn your name above your head no star will flame one weary sound will be the the hoarse roar of the gale the shadows fall from your tired eyes as your lone bedside candle dies for here the calendar breeds nights till stores of candles fail what prompts this melancholy key a long familiar melody it sounds again so let it be let it sound from this night let it sound in my hour of as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky you glare in silence at the wall your stocking gapes no gifts at all its clear that you are now too old to trust in good saint nick that its too late for miracles suddenly lifting your eyes to heavens light you realize your life is a sheer gift END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Title  \\\n",
       "13829          START january END   \n",
       "13831  START the bean eaters END   \n",
       "13832       START the spider END   \n",
       "13833    START the distances END   \n",
       "13848          START january END   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Poem  \n",
       "13829                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    START the wise men will unlearn your name above your head no star will flame one weary sound will be the the hoarse roar of the gale the shadows fall from your tired eyes as your lone bedside candle dies for here the calendar breeds nights till stores of candles fail what prompts this melancholy key a long familiar melody it sounds again so let it be let it sound from this night let it sound in my hour of as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky you glare in silence at the wall your stocking gapes no gifts at all its clear that you are now too old to trust in good saint nick that its too late for miracles suddenly lifting your eyes to heavens light you realize your life is a sheer gift END  \n",
       "13831                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     START they eat beans mostly this old yellow pair dinner is a casual affair plain chipware on a plain and creaking wood tin flatware two who are mostly good two who have lived their day but keep on putting on their clothes and putting things away and remembering remembering with twinklings and twinges as they lean over the beans in their rented back room that is full of beads and receipts and dolls and cloths tobacco crumbs vases and fringes END  \n",
       "13832                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            START i the spider expects the cold of winter when the shadows fall in long autumn he congeals in a nest of paper prepares the least and minimal existence obedient to nature no other course is his no other availed him when in high summer he spun and furled the gaudy catches i am that spider caught in nature summer and winter you are the symbol of the seasons too ii now to expatiate and temporize this artful brag i never saw so quieting a sight as the dawn wide web hung on summer spangled it moves to zephyrs that is tough as steel i never saw so a creature walk so accurate a stretch as he proud capable patient confident to the eye he gave close penetration into real myth the myth of you of me iii yet by moving eyesight off from this there is another dimension near the barn down meadow to shingle no place for spiders the sea in large blue breathes in brainstorm tides pirates itself away to ancient spain pirouettes past purgatory to paradise do i feed deeper on a spider a view upon windless meaning or deeper a day or dance or doom bestride on ocean s long reach on parables of god END  \n",
       "13833  START the accumulation of reefs piling up one over the others like thoughts of the sky increasing as the head rises unto horizons of wet december days perforated with idle motions of gulls and our feelings i ve been wondering about what you mean standing in the spray of shadows before an ocean abandoned for winter silent as a barque of blond hair and the way the clouds are bending the way they react to your position where your hands close over your breasts like an eyelid approving the opening of an evening s parasites attach themselves to the moss covering your feet blind cubans tossing pearls across the jetty and the sound of blood fixes our eyes on the red waves it is a shark and our love is that rusted bottle pointing north the direction which we turn conjuring up our silver knives and spoons and erasing messages in the sand where you wrote freezing in the arctic of our dreams and i said yes delaying the cold medium for a time while you continued to cultivate our possessions as the moon probably continued to cradle tan below the slant of all those wasted trees while the scent carried us back to where we were dancing like the children of great diplomats with our lean bodies draped in bedsheets and leather flags while the orchestra made sounds which we thought was the sky but was only a series of words dying in the thick falsetto of mist for what can anyone create from all these things the fancied tilt of stars sordid doves burning in the hollow brick oven oceans which generalize tears it is known to us in immediate gestures like candle drippings on a silk floor what are we going to do with anything besides pick it up gently and lay it on the breath of still another morning mornings which are always remaining behind for one thing or another shivering in our faces of pride and blooming attitude in the draught of winter air my horse is screaming you are welcoming the new day with your hair leaning against the sand feet dive like otters in the frost and the sudden blue seems to abandon as you leap o to make everything summer soldiers move along lines like wet motions in the violent shade s reappearance but what if your shadow no longer extends to my sleeping and your youth dissolves in my hand like a tongue as the squandered oceans and skies will dissolve into a single plane so i ll move along that plane unnoticed and gray as a drift of skulls over the cool atlantic where i am standing now defining you in perhaps the only word i can as other words are appearing so cunningly on the lips of the many strips of light like naked bodies stretched out along the only beach that remained brown and perfect below the descending of tides END  \n",
       "13848                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    START the wise men will unlearn your name above your head no star will flame one weary sound will be the the hoarse roar of the gale the shadows fall from your tired eyes as your lone bedside candle dies for here the calendar breeds nights till stores of candles fail what prompts this melancholy key a long familiar melody it sounds again so let it be let it sound from this night let it sound in my hour of as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky you glare in silence at the wall your stocking gapes no gifts at all its clear that you are now too old to trust in good saint nick that its too late for miracles suddenly lifting your eyes to heavens light you realize your life is a sheer gift END  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6047bbd-4793-4e2f-8253-475883f55e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title    0\n",
       "Poem     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed98f17a-a9cc-4c07-ac3e-73410277b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109038\n"
     ]
    }
   ],
   "source": [
    "# Tokenize titles and poems\n",
    "tokenized_titles = df['Title'].apply(word_tokenize)\n",
    "tokenized_poems = df['Poem'].apply(word_tokenize)\n",
    "\n",
    "# Collect all tokens, including punctuation, and special characters\n",
    "all_tokens = []\n",
    "for title_words in tokenized_titles:\n",
    "    all_tokens.extend(title_words)\n",
    "for poem_words in tokenized_poems:\n",
    "    all_tokens.extend(poem_words)\n",
    "\n",
    "# Create the unique vocabulary, including special tokens\n",
    "unique_tokens = list(set(all_tokens))\n",
    "special_tokens = [\"UNK\", \"PAD\"]\n",
    "vocabulary = special_tokens + unique_tokens\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4d92899-2369-495b-8820-4fdd55598d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.' in unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6992223-63e9-4526-9225-4581f07c5f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13618"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Title'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc71826-b2db-4607-96cb-fdb8fe327305",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m----> 5\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocabulary)  \u001b[38;5;66;03m# Number of unique words in the vocabulary\u001b[39;00m\n\u001b[0;32m      6\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m105\u001b[39m  \u001b[38;5;66;03m# Number of dimensions for word embeddings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m context_window_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Number of surrounding words considered for context (adjust)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "vocab_size = len(vocabulary)  # Number of unique words in the vocabulary\n",
    "embedding_dim = 105  # Number of dimensions for word embeddings\n",
    "context_window_size = 5  # Number of surrounding words considered for context (adjust)\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "pad_index = 1\n",
    "device = \"cuda\"\n",
    "\n",
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "    def forward(self, titles):\n",
    "        batch_size, total_length = titles.shape \n",
    "        \n",
    "        # Initialize result tensor filled with padding value, here assumed as ones for simplicity\n",
    "        # You might choose a different value or method depending on how you handle padding in embeddings\n",
    "        result_embeddings = torch.ones(batch_size, total_length - (2 * context_window_size), embedding_dim)\n",
    "            \n",
    "        for batch in range(batch_size):\n",
    "            title = titles[batch]\n",
    "    \n",
    "            # Find indices that are not padding\n",
    "            non_pad_indices = (title != pad_index).nonzero().squeeze()\n",
    "            \n",
    "            for idx, i in enumerate(non_pad_indices):\n",
    "                if i - self.context_window_size < 0 or i + self.context_window_size >= total_length:\n",
    "                    # Skip positions where full context cannot be obtained due to padding\n",
    "                    continue\n",
    "                    \n",
    "                # Gather context indices, avoiding going out of bounds\n",
    "                context_indices_left = title[max(i - self.context_window_size, 0):i]\n",
    "                context_indices_right = title[i+1:min(i+1+self.context_window_size, total_length)]\n",
    "                context_indices = torch.cat((context_indices_left, context_indices_right), dim=0).unsqueeze(0)\n",
    "                \n",
    "                # Compute and store the average context embedding at the appropriate position\n",
    "                context_embedding = self.embeddings(context_indices).mean(dim=1)\n",
    "                result_embeddings[batch, i, :] = context_embedding\n",
    "        \n",
    "        return result_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7289b7db-2d6e-495a-b652-04adfa320ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_embedder = WordEmbedder(vocab_size, embedding_dim, context_window_size).to(device)\n",
    "# print(word_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b85d5c4-2744-4ae2-825b-70abd422108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest title: 15617\n"
     ]
    }
   ],
   "source": [
    "longest_length = 0\n",
    "longest_title = None\n",
    "\n",
    "for seq in tokenized_poems:\n",
    "    if len(seq) > longest_length:\n",
    "        longest_length = len(seq)\n",
    "        longest_seq = seq\n",
    "\n",
    "print(\"Length of the longest title:\", longest_length)\n",
    "# print(\"The longest seq:\", ' '.join(longest_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c163508-045d-4b2e-b9e2-364eb5ee1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of padded inputs tensor: torch.Size([27236, 15627])\n"
     ]
    }
   ],
   "source": [
    "# We map word to indexes instead of one-hot encoding as it takes up less space and is faster\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Add context_window_size as a factor to longest length \n",
    "longest_length_plus_context = longest_length + (2 * context_window_size)\n",
    "\n",
    "# Replace all words in the titles and poems with their respective token indexes\n",
    "# If a word is not in the word_to_index dictionary, replace it with 0 (index of UNK)\n",
    "indexed_titles = [[word_to_index.get(word, word_to_index[\"UNK\"]) for word in title] for title in tokenized_titles]\n",
    "indexed_poems = [[word_to_index.get(word, word_to_index[\"UNK\"]) for word in poem] for poem in tokenized_poems]\n",
    "\n",
    "# Apply padding\n",
    "pad_token = word_to_index[\"PAD\"]\n",
    "padded_titles = torch.tensor([[pad_token] * context_window_size + title + [pad_token] * (longest_length_plus_context - len(title) - context_window_size) for title in indexed_titles]).to(device)\n",
    "padded_poems = torch.tensor([[pad_token] * context_window_size + poem + [pad_token] * (longest_length_plus_context - len(poem) - context_window_size) for poem in indexed_poems]).to(device)\n",
    "\n",
    "# Concatenate titles and poems one below the other\n",
    "padded_input = torch.cat((padded_titles, padded_poems), dim=0).to(device)\n",
    "\n",
    "# Create PyTorch dataset and dataloader\n",
    "input_dataset = TensorDataset(padded_input)\n",
    "input_data_loader = DataLoader(input_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Verify shapes (optional)\n",
    "print(f\"Shape of padded inputs tensor: {padded_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b44f04-32fd-40cc-bad7-20e31826e4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15617"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2e72b79-940c-4aac-9e26-6f4428469425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in input_data_loader:\n",
    "#     print(i[0].shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99aea03e-a973-4cca-9212-fe73ca4522ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index[\"UNK\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00f7f851-312e-4b53-a13f-857f8cc81313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles_with_unk = [title for title in indexed_titles if 0 in title]\n",
    "titles_with_unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6045c-3b2f-427e-9f64-f673ff42839b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|                                                                          | 0/2724 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 15  # Number of epochs to train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(input_data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        titles = batch[0].to(device)  # Move the batch of titles to the specified device (e.g., GPU)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset the gradients to zero before the backward pass\n",
    "\n",
    "        # Compute the predicted context embeddings for the current batch\n",
    "        predicted_context_embeddings = word_embedder(titles)\n",
    "        # Prepare the target embeddings\n",
    "        # Assuming titles[:, context_window_size:-context_window_size] are the target indices\n",
    "        target_indices = titles[:, context_window_size:-context_window_size]\n",
    "        target_embeddings = word_embedder.embeddings(target_indices)\n",
    "        # The target embeddings have a shape of [batch_size, reduced_seq_len, embedding_dim]\n",
    "\n",
    "        # Create a mask to exclude padding and edge tokens from the loss calculation\n",
    "        pad_mask = (titles != pad_index)[:, context_window_size:-context_window_size].unsqueeze(-1)\n",
    "        pad_mask = pad_mask.expand(-1, -1, embedding_dim)  # Expand the mask to cover the embedding dimensions\n",
    "\n",
    "        # Apply the mask to the predicted and target embeddings\n",
    "        # This ensures that the loss is only computed for the relevant (non-padding) tokens\n",
    "        predicted_context_embeddings *= pad_mask\n",
    "        target_embeddings *= pad_mask\n",
    "\n",
    "        # Compute the loss between the predicted and target embeddings\n",
    "        # Note: Depending on the specific requirements, you may need to adjust the way the loss is computed\n",
    "        # to handle the 3D nature of the tensors and potential misalignment\n",
    "        loss = loss_fn(predicted_context_embeddings, target_embeddings)\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Compute the average loss for the current epoch\n",
    "    average_loss = total_loss / len(input_data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97ec5fef-72c5-4524-b4b2-2c3df5905682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have successfully trained our word embedding layer, time to make our positional matrix, which we will make using the formula mentioned in the paper\n",
    "def positional_encoding(seq_len, embedding_dim, device=\"cuda\"):\n",
    "    position = torch.arange(seq_len).unsqueeze(1).float().to(device)\n",
    "    div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(math.log(10000.0) / embedding_dim)).to(device)\n",
    "\n",
    "    pos_encoding = torch.zeros(seq_len, embedding_dim).to(device)\n",
    "    pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "    pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fa380b5-1465-483c-b1ea-562a7978bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15617, 16])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder_pos_encoding = positional_encoding(longest_length, embedding_dim)\n",
    "# encoder_pos_encoding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6f80c9-fd5a-4014-8750-3724b592e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to make the Multi-head Self Attention block\n",
    "heads = 3\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Calculate attention\n",
    "        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(embedding_dim)), V)\n",
    "\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.transpose(1, 2).view(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0155ef-eee5-41c3-8c1b-48d9f40dc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to make the Masked Multi-head Self Attention block\n",
    "class MaskedMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, heads, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "        # 3 Linear Layers for Q, K and V\n",
    "        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "        # Since the feature or embedding dimension is typically the last dimension\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        # Last Linear layer for the attention\n",
    "        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, embedding_vector):\n",
    "        batch_size, seq_len, embedding_dim = embedding_vector.size()\n",
    "        # Compute Q, K, and V\n",
    "        Q = self.w_q(embedding_vector)\n",
    "        K = self.w_k(embedding_vector)\n",
    "        V = self.w_v(embedding_vector)\n",
    "\n",
    "        # Seperate into heads\n",
    "        head_dim = embedding_dim // self.heads\n",
    "        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        K = K.view(batch_size, self.heads, seq_len, head_dim)\n",
    "        V = V.view(batch_size, self.heads, seq_len, head_dim)\n",
    "\n",
    "        # Create a mask for masking the attention score\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).unsqueeze(0).unsqueeze(0).expand(batch_size, self.heads, -1, -1)\n",
    "        value_to_fill = float('-inf')\n",
    "        \n",
    "        # Calculate attention (including mask)\n",
    "        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)).masked_fill(mask, value_to_fill) / torch.sqrt(embedding_dim)), V)\n",
    "\n",
    "        # Concatenating the attention heads (Transposing for correct concatenation)\n",
    "        attention = attention.transpose(1, 2).view(batch_size, seq_len, embedding_dim)\n",
    "        output = self.w_a(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ecdfc-9f1c-4120-8578-46e05d1c2069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        # Layer Norm will normalize the last dimension of the matrix\n",
    "        self.norm = nn.LayerNorm(n_features)\n",
    "\n",
    "    def forward(original, modified):\n",
    "        matrix = original + modified\n",
    "        out = self.norm(matrix)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5b4a5-96f7-40ba-9193-ab6e0dd7ccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super.__init__()\n",
    "        # Normally nn.Linear(embedding_dim, embedding_dim * 4) for expressiveness, we will change it if we have the resources to do so\n",
    "        self.lr1 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.lr2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1f5ed-35ce-479b-b596-d2cc95a2c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to build the Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size, batch_size, heads):\n",
    "        super().__init__()\n",
    "        self.word_embedder = WordEmbedder(vocab_size, embedding_dim, context_window_size).to(device)\n",
    "        self.positional_encoding = positional_encoding(longest_length + (2 * context_window_size), embedding_dim).to(device)\n",
    "        self.masked_attention = MaskedMultiHeadSelfAttention(heads, embedding_dim).to(device)\n",
    "        self.add_norm1 = AddNorm(embedding_dim).to(device)\n",
    "        self.attention = MultiHeadSelfAttention(heads, embedding_dim).to(device)\n",
    "        self.add_norm2 = AddNorm(embedding_dim).to(device)        \n",
    "        self.feed_forward = FeedForward(embedding_dim).to(device)\n",
    "        self.add_norm3 = AddNorm(embedding_dim).to(device)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.softmax = F.softmax(vocab_size, dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.word_embedder(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.masked_attention(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026a026-a1e7-4f12-9f82-8a78982bb1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our optimizer and loss function\n",
    "optimizer = torch.optim.Adam(word_embedder.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881cd445-c7fb-4f2b-ae1f-d6807b2f76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_size, embedding_dim, context_window_size, batch_size, heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdc8bd-74e2-45f3-87ea-03c882aa2dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
