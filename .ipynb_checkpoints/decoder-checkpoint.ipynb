{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":502516,"sourceType":"datasetVersion","datasetId":236282}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom nltk.tokenize import word_tokenize\nimport torch\nimport math\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"id":"7e893663-462c-4e19-98f0-2611f454ccf7","outputId":"61e055bc-b2ed-4462-c68e-32e168b344a7","execution":{"iopub.status.busy":"2024-04-15T07:04:28.401225Z","iopub.execute_input":"2024-04-15T07:04:28.401511Z","iopub.status.idle":"2024-04-15T07:04:34.798512Z","shell.execute_reply.started":"2024-04-15T07:04:28.401486Z","shell.execute_reply":"2024-04-15T07:04:34.797657Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/poetry-foundation-poems/PoetryFoundationData.csv\")","metadata":{"id":"46a99932-f65a-4ffd-981b-4eb2c1c32775","execution":{"iopub.status.busy":"2024-04-15T07:04:34.800054Z","iopub.execute_input":"2024-04-15T07:04:34.800458Z","iopub.status.idle":"2024-04-15T07:04:35.467658Z","shell.execute_reply.started":"2024-04-15T07:04:34.800432Z","shell.execute_reply":"2024-04-15T07:04:35.466749Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_colwidth', None)\ndf.tail()","metadata":{"id":"259e65ac-45c6-452d-b220-6f9e3f605093","outputId":"58475cf0-ef4a-48e1-95e2-8e142b742461","execution":{"iopub.status.busy":"2024-04-15T07:04:35.469132Z","iopub.execute_input":"2024-04-15T07:04:35.469502Z","iopub.status.idle":"2024-04-15T07:04:35.486664Z","shell.execute_reply.started":"2024-04-15T07:04:35.469474Z","shell.execute_reply":"2024-04-15T07:04:35.485692Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       Unnamed: 0  \\\n13849          13   \n13850          14   \n13851          15   \n13852           0   \n13853           1   \n\n                                                                         Title  \\\n13849               \\r\\r\\n                    1-800-FEAR\\r\\r\\n                   \n13850  \\r\\r\\n                    The Death of Atahuallpa\\r\\r\\n                   \n13851              \\r\\r\\n                    Poet's Wish\\r\\r\\n                   \n13852                        \\r\\r\\n                    0\\r\\r\\n                   \n13853                        \\r\\r\\n                    !\\r\\r\\n                   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Poem  \\\n13849  \\r\\r\\nWe'd  like  to  talk  with  you  about  fear they  said  so\\r\\r\\r\\nmany  people  live  in  fear  these  days  they  drove  up\\r\\r\\r\\nall  four  of  them  in  a  small  car nice   boy  they  said\\r\\r\\r\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\r\\r\\r\\nof  the  woman  the other  two  waiting  in  the  drive  I\\r\\r\\r\\nwas  outside digging up the garden no one home I said\\r\\r\\r\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\r\\r\\r\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\r\\r\\r\\ncard  there's   a  phone  number  you  can  call  anytime\\r\\r\\r\\nany  other   houses  down  this  road  anyone  else   live\\r\\r\\r\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\\r\\r\\n   \n13850                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\r\\r\\n\\r\\r\\n   \n13851                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \\r\\r\\n\\r\\r\\n   \n13852                                                                                                                                                                                                         \\r\\r\\n          Philosophic\\r\\r\\nin its complex, ovoid emptiness,\\r\\r\\na skillful pundit coined it as a sort\\r\\r\\n    of stopgap doorstop for those\\r\\r\\n           quaint equations           Romans never\\r\\r\\ndreamt of. In form completely clever\\r\\r\\nand discrete—a mirror come unsilvered,     loose watch face without the works,              a hollowed globe            from tip to toe\\r\\r\\nunbroken, it evades the grappling\\r\\r\\nhooks of mass, tilts the thin rim of no thing,     remains embryonic sum,             non-cogito.\\r\\r\\n   \n13853                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \\r\\r\\nDear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.     \n\n                    Poet  \\\n13849      Jody Gladding   \n13850  William Jay Smith   \n13851  William Jay Smith   \n13852  Hailey Leithauser   \n13853     Wendy Videlock   \n\n                                                                                                          Tags  \n13849                                                               Living,Social Commentaries,Popular Culture  \n13850                                                                                                      NaN  \n13851                                                                                                      NaN  \n13852                                                                               Arts & Sciences,Philosophy  \n13853  Relationships,Gay, Lesbian, Queer,Arts & Sciences,Poetry & Poets,Social Commentaries,Gender & Sexuality  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Title</th>\n      <th>Poem</th>\n      <th>Poet</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13849</th>\n      <td>13</td>\n      <td>\\r\\r\\n                    1-800-FEAR\\r\\r\\n</td>\n      <td>\\r\\r\\nWe'd  like  to  talk  with  you  about  fear they  said  so\\r\\r\\r\\nmany  people  live  in  fear  these  days  they  drove  up\\r\\r\\r\\nall  four  of  them  in  a  small  car nice   boy  they  said\\r\\r\\r\\nbeautiful  dogs they  said  so  friendly  the  man  ahead\\r\\r\\r\\nof  the  woman  the other  two  waiting  in  the  drive  I\\r\\r\\r\\nwas  outside digging up the garden no one home I said\\r\\r\\r\\nwhat   are  you  selling   anyway  I'm   not  interested  I\\r\\r\\r\\nsaid  well  you  have  a  nice  day  they  said  here's  our\\r\\r\\r\\ncard  there's   a  phone  number  you  can  call  anytime\\r\\r\\r\\nany  other   houses  down  this  road  anyone  else   live\\r\\r\\r\\nhere  we'd  like  to  talk  to  them  about  living  in  fear\\r\\r\\n</td>\n      <td>Jody Gladding</td>\n      <td>Living,Social Commentaries,Popular Culture</td>\n    </tr>\n    <tr>\n      <th>13850</th>\n      <td>14</td>\n      <td>\\r\\r\\n                    The Death of Atahuallpa\\r\\r\\n</td>\n      <td>\\r\\r\\n\\r\\r\\n</td>\n      <td>William Jay Smith</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13851</th>\n      <td>15</td>\n      <td>\\r\\r\\n                    Poet's Wish\\r\\r\\n</td>\n      <td>\\r\\r\\n\\r\\r\\n</td>\n      <td>William Jay Smith</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13852</th>\n      <td>0</td>\n      <td>\\r\\r\\n                    0\\r\\r\\n</td>\n      <td>\\r\\r\\n          Philosophic\\r\\r\\nin its complex, ovoid emptiness,\\r\\r\\na skillful pundit coined it as a sort\\r\\r\\n    of stopgap doorstop for those\\r\\r\\n           quaint equations           Romans never\\r\\r\\ndreamt of. In form completely clever\\r\\r\\nand discrete—a mirror come unsilvered,     loose watch face without the works,              a hollowed globe            from tip to toe\\r\\r\\nunbroken, it evades the grappling\\r\\r\\nhooks of mass, tilts the thin rim of no thing,     remains embryonic sum,             non-cogito.\\r\\r\\n</td>\n      <td>Hailey Leithauser</td>\n      <td>Arts &amp; Sciences,Philosophy</td>\n    </tr>\n    <tr>\n      <th>13853</th>\n      <td>1</td>\n      <td>\\r\\r\\n                    !\\r\\r\\n</td>\n      <td>\\r\\r\\nDear Writers, I’m compiling the first in what I hope is a series of publications I’m calling artists among artists. The theme for issue 1 is “Faggot Dinosaur.” I hope to hear from you! Thank you and best wishes.</td>\n      <td>Wendy Videlock</td>\n      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Sciences,Poetry &amp; Poets,Social Commentaries,Gender &amp; Sexuality</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"68c8a2b5-553f-494e-bed7-b87dda4a31a0","outputId":"da4a73cb-8836-426c-d431-512c1d447694","execution":{"iopub.status.busy":"2024-04-15T07:04:35.488702Z","iopub.execute_input":"2024-04-15T07:04:35.488969Z","iopub.status.idle":"2024-04-15T07:04:35.503706Z","shell.execute_reply.started":"2024-04-15T07:04:35.488946Z","shell.execute_reply":"2024-04-15T07:04:35.502783Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0      0\nTitle           0\nPoem            0\nPoet            0\nTags          955\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df = df.drop(['Unnamed: 0', 'Tags', 'Poet', 'Title'], axis=1)","metadata":{"id":"bfb30a34-a0f3-4202-b652-931b29a215a1","execution":{"iopub.status.busy":"2024-04-15T07:04:35.504804Z","iopub.execute_input":"2024-04-15T07:04:35.505050Z","iopub.status.idle":"2024-04-15T07:04:35.513657Z","shell.execute_reply.started":"2024-04-15T07:04:35.505028Z","shell.execute_reply":"2024-04-15T07:04:35.512772Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# df['Title'] = df['Title'].str.replace('\\r', '')\ndf['Poem'] = df['Poem'].str.replace('\\r', '')\n# df['Title'] = df['Title'].str.replace('\\n', '')\ndf['Poem'] = df['Poem'].str.strip('\\n')","metadata":{"id":"744c8e4b-7444-4c88-bceb-d1986680a5da","execution":{"iopub.status.busy":"2024-04-15T07:04:35.514667Z","iopub.execute_input":"2024-04-15T07:04:35.514898Z","iopub.status.idle":"2024-04-15T07:04:35.633221Z","shell.execute_reply.started":"2024-04-15T07:04:35.514878Z","shell.execute_reply":"2024-04-15T07:04:35.632420Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Lower case everything\n# df['Title'] = df['Title'].str.lower()\ndf['Poem'] = df['Poem'].str.lower()\n\n# Remove apostrophes and join the parts\n# df['Title'] = df['Title'].str.replace(\"'\", \"\", regex=False)\ndf['Poem'] = df['Poem'].str.replace(\"'\", \"\", regex=False)","metadata":{"id":"147cacee-0272-4883-a8b0-5e9f9cc42621","execution":{"iopub.status.busy":"2024-04-15T07:04:35.634234Z","iopub.execute_input":"2024-04-15T07:04:35.634496Z","iopub.status.idle":"2024-04-15T07:04:35.864900Z","shell.execute_reply.started":"2024-04-15T07:04:35.634474Z","shell.execute_reply":"2024-04-15T07:04:35.863973Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Remove tokens containing numbers from the Titles\ndef remove_numeric_tokens(text):\n    tokens = word_tokenize(text)\n    filtered_tokens = [token for token in tokens if token.isalpha()]\n    processed_text = ' '.join(filtered_tokens)\n    return processed_text\n\n# df['Title'] = df['Title'].apply(remove_numeric_tokens)\ndf['Poem'] = df['Poem'].apply(remove_numeric_tokens)","metadata":{"id":"255a982a-7b39-49eb-814f-c56e15fc6b44","execution":{"iopub.status.busy":"2024-04-15T07:04:35.866171Z","iopub.execute_input":"2024-04-15T07:04:35.866467Z","iopub.status.idle":"2024-04-15T07:05:17.518977Z","shell.execute_reply.started":"2024-04-15T07:04:35.866442Z","shell.execute_reply":"2024-04-15T07:05:17.517970Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Uncomment only when including encoder, until then let it be commented off\n# Filter out rows where both 'Title' and 'Poem' have no alphabetic characters\n\n# df = df[df.apply(lambda x: any(c.isalpha() for c in x['Title']) and\n#                             any(c.isalpha() for c in x['Poem']), axis=1)]\n\n# Filter out rows with no value (its not NaN its '' in this dataset)\ndf = df[df.apply(lambda x: any(c.isalpha() for c in x['Poem']), axis=1)]","metadata":{"id":"d53c2016-cbe5-4bcf-9a11-94f7abf27a6a","execution":{"iopub.status.busy":"2024-04-15T07:05:17.520357Z","iopub.execute_input":"2024-04-15T07:05:17.520676Z","iopub.status.idle":"2024-04-15T07:05:17.672933Z","shell.execute_reply.started":"2024-04-15T07:05:17.520651Z","shell.execute_reply":"2024-04-15T07:05:17.672163Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Make newline a special token\n# df['Title'] = 'START ' + df['Title'] + ' END'\ndf['Poem'] = 'START ' + df['Poem'] + ' END'\n\ndf['Poem'] = df['Poem'].str.replace('\\n', ' N ')","metadata":{"id":"54d2c668-0a97-4e00-8976-f5260fb83bd7","outputId":"f54c2dde-9fba-4202-ab82-9fa8cd55c526","execution":{"iopub.status.busy":"2024-04-15T07:05:17.675554Z","iopub.execute_input":"2024-04-15T07:05:17.675870Z","iopub.status.idle":"2024-04-15T07:05:17.710112Z","shell.execute_reply.started":"2024-04-15T07:05:17.675845Z","shell.execute_reply":"2024-04-15T07:05:17.709244Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"id":"14b93fd6-d251-4d70-9bfb-063ee537fa3b","outputId":"29dfabbb-ed2c-4829-9256-be7f796bee2b","execution":{"iopub.status.busy":"2024-04-15T07:05:17.711172Z","iopub.execute_input":"2024-04-15T07:05:17.711468Z","iopub.status.idle":"2024-04-15T07:05:17.725353Z","shell.execute_reply.started":"2024-04-15T07:05:17.711444Z","shell.execute_reply":"2024-04-15T07:05:17.724411Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Poem\n13835                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        START dear writers i m compiling the first in what i hope is a series of publications i m calling artists among artists the theme for issue is faggot i hope to hear from you thank you and best wishes END\n13848  START the wise men will unlearn your name above your head no star will flame one weary sound will be the the hoarse roar of the gale the shadows fall from your tired eyes as your lone bedside candle dies for here the calendar breeds nights till stores of candles fail what prompts this melancholy key a long familiar melody it sounds again so let it be let it sound from this night let it sound in my hour of as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky you glare in silence at the wall your stocking gapes no gifts at all its clear that you are now too old to trust in good saint nick that its too late for miracles suddenly lifting your eyes to heavens light you realize your life is a sheer gift END\n13849                                                                                                                                                                                                   START wed like to talk with you about fear they said so many people live in fear these days they drove up all four of them in a small car nice boy they said beautiful dogs they said so friendly the man ahead of the woman the other two waiting in the drive i was outside digging up the garden no one home i said what are you selling anyway im not interested i said well you have a nice day they said heres our card theres a phone number you can call anytime any other houses down this road anyone else live here wed like to talk to them about living in fear END\n13852                                                                                                                                                                                                                                                                                                                                                                                      START philosophic in its complex ovoid emptiness a skillful pundit coined it as a sort of stopgap doorstop for those quaint equations romans never dreamt of in form completely clever and mirror come unsilvered loose watch face without the works a hollowed globe from tip to toe unbroken it evades the grappling hooks of mass tilts the thin rim of no thing remains embryonic sum END\n13853                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        START dear writers i m compiling the first in what i hope is a series of publications i m calling artists among artists the theme for issue is faggot i hope to hear from you thank you and best wishes END","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Poem</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>13835</th>\n      <td>START dear writers i m compiling the first in what i hope is a series of publications i m calling artists among artists the theme for issue is faggot i hope to hear from you thank you and best wishes END</td>\n    </tr>\n    <tr>\n      <th>13848</th>\n      <td>START the wise men will unlearn your name above your head no star will flame one weary sound will be the the hoarse roar of the gale the shadows fall from your tired eyes as your lone bedside candle dies for here the calendar breeds nights till stores of candles fail what prompts this melancholy key a long familiar melody it sounds again so let it be let it sound from this night let it sound in my hour of as gratefulness of eyes and lips for that which sometimes makes us lift our gaze to the far sky you glare in silence at the wall your stocking gapes no gifts at all its clear that you are now too old to trust in good saint nick that its too late for miracles suddenly lifting your eyes to heavens light you realize your life is a sheer gift END</td>\n    </tr>\n    <tr>\n      <th>13849</th>\n      <td>START wed like to talk with you about fear they said so many people live in fear these days they drove up all four of them in a small car nice boy they said beautiful dogs they said so friendly the man ahead of the woman the other two waiting in the drive i was outside digging up the garden no one home i said what are you selling anyway im not interested i said well you have a nice day they said heres our card theres a phone number you can call anytime any other houses down this road anyone else live here wed like to talk to them about living in fear END</td>\n    </tr>\n    <tr>\n      <th>13852</th>\n      <td>START philosophic in its complex ovoid emptiness a skillful pundit coined it as a sort of stopgap doorstop for those quaint equations romans never dreamt of in form completely clever and mirror come unsilvered loose watch face without the works a hollowed globe from tip to toe unbroken it evades the grappling hooks of mass tilts the thin rim of no thing remains embryonic sum END</td>\n    </tr>\n    <tr>\n      <th>13853</th>\n      <td>START dear writers i m compiling the first in what i hope is a series of publications i m calling artists among artists the theme for issue is faggot i hope to hear from you thank you and best wishes END</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.isna().sum()","metadata":{"id":"b6047bbd-4793-4e2f-8253-475883f55e9d","outputId":"db82f9b3-d82f-4f8c-d1fd-1aa8eaeaeed2","execution":{"iopub.status.busy":"2024-04-15T07:05:17.726639Z","iopub.execute_input":"2024-04-15T07:05:17.726982Z","iopub.status.idle":"2024-04-15T07:05:17.738447Z","shell.execute_reply.started":"2024-04-15T07:05:17.726953Z","shell.execute_reply":"2024-04-15T07:05:17.737570Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Poem    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df.size","metadata":{"id":"9e54db8f-fd00-4736-972c-95dbfb990694","outputId":"f57c1f35-ecd6-41a3-a1cc-5d6f1b568206","execution":{"iopub.status.busy":"2024-04-15T07:05:17.739581Z","iopub.execute_input":"2024-04-15T07:05:17.739855Z","iopub.status.idle":"2024-04-15T07:05:17.748374Z","shell.execute_reply.started":"2024-04-15T07:05:17.739833Z","shell.execute_reply":"2024-04-15T07:05:17.747578Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"13750"},"metadata":{}}]},{"cell_type":"code","source":"length_to_trim = 20\n# Tokenize poems\ntokenized_poems = df['Poem'].apply(word_tokenize)\n\n# Get the lengths of all poems\npoem_lengths = [len(seq) for seq in tokenized_poems]\n\n# Sort the lengths in descending order and find the minimum length to keep\nsorted_lengths = sorted(poem_lengths, reverse=True)\nprint(\"Length of the longest poem:\", sorted_lengths[0])\nprint(\"Five longest poem sizes:\", sorted_lengths[:5])\n\n# Determine the maximum length of poem to keep\nlongest_length = sorted_lengths[length_to_trim]\nprint(\"Maximum length for trimming:\", longest_length)\n\n# Filter out poems that are longer than the threshold\ntokenized_poems = [seq for seq, length in zip(tokenized_poems, poem_lengths) if length <= longest_length]\n\n# Collect all tokens from the remaining poems\nall_tokens = [token for poem in tokenized_poems for token in poem]\n\n# Create the unique vocabulary, including special tokens\nunique_tokens = list(set(all_tokens))\nspecial_tokens = [\"UNK\", \"PAD\"]\nvocabulary = special_tokens + unique_tokens\n\nprint(\"Total vocabulary size:\", len(vocabulary))","metadata":{"id":"ed98f17a-a9cc-4c07-ac3e-73410277b1fe","outputId":"789a3b45-dac2-4486-a523-39bb3d97fcec","execution":{"iopub.status.busy":"2024-04-15T07:05:17.749830Z","iopub.execute_input":"2024-04-15T07:05:17.750062Z","iopub.status.idle":"2024-04-15T07:05:42.611907Z","shell.execute_reply.started":"2024-04-15T07:05:17.750042Z","shell.execute_reply":"2024-04-15T07:05:42.611011Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Length of the longest poem: 15617\nFive longest poem sizes: [15617, 9887, 9196, 9035, 8360]\nMaximum length for trimming: 5034\nTotal vocabulary size: 104643\n","output_type":"stream"}]},{"cell_type":"code","source":"'.' in unique_tokens","metadata":{"id":"a4d92899-2369-495b-8820-4fdd55598d02","outputId":"142e0feb-9c55-4595-fb99-f94af4d5f10d","execution":{"iopub.status.busy":"2024-04-14T19:48:03.668461Z","iopub.execute_input":"2024-04-14T19:48:03.668843Z","iopub.status.idle":"2024-04-14T19:48:03.680457Z","shell.execute_reply.started":"2024-04-14T19:48:03.668812Z","shell.execute_reply":"2024-04-14T19:48:03.679448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocabulary)  # Number of unique words in the vocabulary\nembedding_dim = 15  # Number of dimensions for word embeddings\n# context_window_size = 5  # Number of surrounding words considered for context (adjust)\nbatch_size = 1\nlearning_rate = 0.01\npad_index = 1\nheads = 3\nepochs = 2\ndevice = \"cuda\"","metadata":{"id":"0cb421f7-9dac-4651-a2cb-9167ffad6ab4","execution":{"iopub.status.busy":"2024-04-14T19:48:03.681818Z","iopub.execute_input":"2024-04-14T19:48:03.682458Z","iopub.status.idle":"2024-04-14T19:48:03.690459Z","shell.execute_reply.started":"2024-04-14T19:48:03.682424Z","shell.execute_reply":"2024-04-14T19:48:03.689637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WordEmbedder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super().__init__()\n        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n\n    def forward(self, x):\n        return self.embeddings(x)","metadata":{"id":"3cc71826-b2db-4607-96cb-fdb8fe327305","execution":{"iopub.status.busy":"2024-04-14T19:48:03.691622Z","iopub.execute_input":"2024-04-14T19:48:03.691887Z","iopub.status.idle":"2024-04-14T19:48:03.700673Z","shell.execute_reply.started":"2024-04-14T19:48:03.691864Z","shell.execute_reply":"2024-04-14T19:48:03.699672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We map word to indexes instead of one-hot encoding as it takes up less space and is faster\nword_to_index = {word: idx for idx, word in enumerate(vocabulary)}\nindex_to_word = {idx: word for word, idx in word_to_index.items()}\n\n# Add context_window_size as a factor to longest length\n# longest_length_plus_context = longest_length + (2 * context_window_size)\n\n# Replace all words in the titles and poems with their respective token indexes\n# If a word is not in the word_to_index dictionary, replace it with 0 (index of UNK)\n# indexed_titles = [[word_to_index.get(word, word_to_index[\"UNK\"]) for word in title] for title in tokenized_titles]\nindexed_poems = [[word_to_index.get(word, word_to_index[\"UNK\"]) for word in poem] for poem in tokenized_poems]\n\n# Apply padding\npad_token = word_to_index[\"PAD\"]\n# padded_titles = torch.tensor([title + [pad_token] * (longest_length - len(title)) for title in indexed_titles]).to(device)\npadded_poems = torch.tensor([poem + [pad_token] * (longest_length - len(poem)) for poem in indexed_poems]).to(device)\n\n# Uncomment when using encoder too\n# Concatenate titles and poems one below the other\n# padded_input = torch.cat((padded_titles, padded_poems), dim=0).to(device)\n\n# Create PyTorch dataset and dataloader\ndataset = TensorDataset(padded_poems)\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Verify shapes (optional)\nprint(f\"Shape of padded inputs tensor: {padded_poems.shape}\")","metadata":{"id":"0c163508-045d-4b2e-b9e2-364eb5ee1344","outputId":"89bfa492-bc5d-4f58-fbab-35be98fa5fa2","execution":{"iopub.status.busy":"2024-04-14T19:48:03.701861Z","iopub.execute_input":"2024-04-14T19:48:03.702138Z","iopub.status.idle":"2024-04-14T19:48:27.548784Z","shell.execute_reply.started":"2024-04-14T19:48:03.702116Z","shell.execute_reply":"2024-04-14T19:48:27.547753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in data_loader:\n#     print(i[0].shape)\n#     break","metadata":{"id":"e2e72b79-940c-4aac-9e26-6f4428469425","execution":{"iopub.status.busy":"2024-04-14T19:48:27.549886Z","iopub.execute_input":"2024-04-14T19:48:27.550159Z","iopub.status.idle":"2024-04-14T19:48:27.554322Z","shell.execute_reply.started":"2024-04-14T19:48:27.550136Z","shell.execute_reply":"2024-04-14T19:48:27.553344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_index[\"UNK\"]","metadata":{"id":"99aea03e-a973-4cca-9212-fe73ca4522ab","outputId":"57ef5631-bfe3-41c8-b15a-98fd627058fb","execution":{"iopub.status.busy":"2024-04-14T19:48:27.555538Z","iopub.execute_input":"2024-04-14T19:48:27.555851Z","iopub.status.idle":"2024-04-14T19:48:27.566930Z","shell.execute_reply.started":"2024-04-14T19:48:27.555827Z","shell.execute_reply":"2024-04-14T19:48:27.566055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"poems_with_unk = [poem for poem in indexed_poems if 0 in poem]\npoems_with_unk","metadata":{"id":"00f7f851-312e-4b53-a13f-857f8cc81313","outputId":"6cc75ad8-5801-463e-b5f3-cacf174de89e","execution":{"iopub.status.busy":"2024-04-14T19:48:27.568069Z","iopub.execute_input":"2024-04-14T19:48:27.568425Z","iopub.status.idle":"2024-04-14T19:48:27.613061Z","shell.execute_reply.started":"2024-04-14T19:48:27.568396Z","shell.execute_reply":"2024-04-14T19:48:27.612219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now that we have successfully trained our word embedding layer, time to make our positional matrix, which we will make using the formula mentioned in the paper\ndef positional_encoding(seq_len, embedding_dim, device=device):\n    pe = torch.zeros(seq_len, embedding_dim, device=device)\n    # Compute the positional encoding values\n    position = torch.arange(0, seq_len, dtype=torch.float, device=device).unsqueeze(1)\n    # Adjust div_term calculation to handle odd embedding_dim\n    div_term_exp = torch.arange(0, embedding_dim, 2).float() * -(np.log(10000.0) / embedding_dim)\n    div_term = torch.exp(div_term_exp).to(device)\n    pe[:, 0::2] = torch.sin(position * div_term)\n    if embedding_dim % 2 == 0:  # Check if embedding_dim is even\n        pe[:, 1::2] = torch.cos(position * div_term)  # For even embedding_dim\n    else:\n        # Adjust the last cosine computation for odd embedding_dim\n        pe[:, 1::2] = torch.cos(position * div_term)[:,:-1]  # Exclude the last column for odd embedding_dim\n    return pe","metadata":{"id":"97ec5fef-72c5-4524-b4b2-2c3df5905682","execution":{"iopub.status.busy":"2024-04-14T19:48:27.614036Z","iopub.execute_input":"2024-04-14T19:48:27.614327Z","iopub.status.idle":"2024-04-14T19:48:27.621810Z","shell.execute_reply.started":"2024-04-14T19:48:27.614274Z","shell.execute_reply":"2024-04-14T19:48:27.620690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_pos_encoding = positional_encoding(longest_length, embedding_dim)\nencoder_pos_encoding.shape","metadata":{"id":"1fa380b5-1465-483c-b1ea-562a7978bef1","outputId":"4444a8ef-2bd6-4ead-a933-5a541c013a90","execution":{"iopub.status.busy":"2024-04-14T19:48:27.622984Z","iopub.execute_input":"2024-04-14T19:48:27.623232Z","iopub.status.idle":"2024-04-14T19:48:27.768670Z","shell.execute_reply.started":"2024-04-14T19:48:27.623210Z","shell.execute_reply":"2024-04-14T19:48:27.767775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to make the Multi-head Self Attention block\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, heads, embedding_dim):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.heads = heads\n\n        # 3 Linear Layers for Q, K and V\n        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n\n        # Since the feature or embedding dimension is typically the last dimension\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Last Linear layer for the attention\n        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, embedding_vector):\n        batch_size, seq_len, embedding_dim = embedding_vector.size()\n        # Compute Q, K, and V\n        Q = self.w_q(embedding_vector)\n        K = self.w_k(embedding_vector)\n        V = self.w_v(embedding_vector)\n\n        # Seperate into heads\n        head_dim = embedding_dim // self.heads\n        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n        K = K.view(batch_size, self.heads, seq_len, head_dim)\n        V = V.view(batch_size, self.heads, seq_len, head_dim)\n\n        # Calculate attention\n        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)) / torch.sqrt(torch.tensor(embedding_dim))), V)\n\n        # Concatenating the attention heads (Transposing for correct concatenation)\n        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n        output = self.w_a(attention)\n        return output","metadata":{"id":"eb6f80c9-fd5a-4014-8750-3724b592e8fd","execution":{"iopub.status.busy":"2024-04-14T19:48:27.769786Z","iopub.execute_input":"2024-04-14T19:48:27.770142Z","iopub.status.idle":"2024-04-14T19:48:27.781264Z","shell.execute_reply.started":"2024-04-14T19:48:27.770107Z","shell.execute_reply":"2024-04-14T19:48:27.780285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to make the Masked Multi-head Self Attention block\nclass MaskedMultiHeadSelfAttention(nn.Module):\n    def __init__(self, heads, embedding_dim):\n        super(MaskedMultiHeadSelfAttention, self).__init__()\n        self.heads = heads\n\n        # 3 Linear Layers for Q, K and V\n        self.w_q = nn.Linear(embedding_dim, embedding_dim)\n        self.w_k = nn.Linear(embedding_dim, embedding_dim)\n        self.w_v = nn.Linear(embedding_dim, embedding_dim)\n\n        # Since the feature or embedding dimension is typically the last dimension\n        self.softmax = nn.Softmax(dim=-1)\n\n        # Last Linear layer for the attention\n        self.w_a = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, embedding_vector):\n        batch_size, seq_len, embedding_dim = embedding_vector.size()\n        # Compute Q, K, and V\n        Q = self.w_q(embedding_vector)\n        K = self.w_k(embedding_vector)\n        V = self.w_v(embedding_vector)\n\n        # Seperate into heads\n        head_dim = embedding_dim // self.heads\n        Q = Q.view(batch_size, self.heads, seq_len, head_dim)\n        K = K.view(batch_size, self.heads, seq_len, head_dim)\n        V = V.view(batch_size, self.heads, seq_len, head_dim)\n\n        # Create a mask for masking the attention score\n        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).unsqueeze(0).unsqueeze(0).expand(batch_size, self.heads, -1, -1).to(device)\n        value_to_fill = float('-inf')\n\n        # Calculate attention (including mask)\n        attention = torch.matmul(self.softmax(torch.matmul(Q, K.transpose(-1, -2)).masked_fill(mask, value_to_fill) / torch.sqrt(torch.tensor(embedding_dim))), V)\n\n        # Concatenating the attention heads (Transposing for correct concatenation)\n        attention = attention.transpose(1, 2).reshape(batch_size, seq_len, embedding_dim)\n        output = self.w_a(attention)\n        return output","metadata":{"id":"6e0155ef-eee5-41c3-8c1b-48d9f40dc0ee","execution":{"iopub.status.busy":"2024-04-14T19:48:27.782780Z","iopub.execute_input":"2024-04-14T19:48:27.783341Z","iopub.status.idle":"2024-04-14T19:48:27.794666Z","shell.execute_reply.started":"2024-04-14T19:48:27.783302Z","shell.execute_reply":"2024-04-14T19:48:27.793814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddNorm(nn.Module):\n    def __init__(self, n_features):\n        super(AddNorm, self).__init__()\n        # Layer Norm will normalize the last dimension of the matrix\n        self.norm = nn.LayerNorm(n_features)\n\n    def forward(self, original, modified):\n        return self.norm(original + modified)","metadata":{"id":"c54ecdfc-9f1c-4120-8578-46e05d1c2069","execution":{"iopub.status.busy":"2024-04-14T19:48:27.795793Z","iopub.execute_input":"2024-04-14T19:48:27.796770Z","iopub.status.idle":"2024-04-14T19:48:27.806149Z","shell.execute_reply.started":"2024-04-14T19:48:27.796745Z","shell.execute_reply":"2024-04-14T19:48:27.805280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, embedding_dim):\n        super(FeedForward, self).__init__()\n        # Normally nn.Linear(embedding_dim, embedding_dim * 4) for expressiveness, we will change it if we have the resources to do so\n        self.lr1 = nn.Linear(embedding_dim, embedding_dim)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(embedding_dim, embedding_dim)\n\n    def forward(self, x):\n        x = self.lr1(x)\n        x = self.relu(x)\n        out = self.lr2(x)\n        return out","metadata":{"id":"a1a5b4a5-96f7-40ba-9193-ab6e0dd7ccfc","execution":{"iopub.status.busy":"2024-04-14T19:48:27.807173Z","iopub.execute_input":"2024-04-14T19:48:27.807426Z","iopub.status.idle":"2024-04-14T19:48:27.817860Z","shell.execute_reply.started":"2024-04-14T19:48:27.807404Z","shell.execute_reply":"2024-04-14T19:48:27.817113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Time to build the Decoder\nclass Decoder(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, batch_size, heads):\n        super(Decoder, self).__init__()\n        self.word_embedder = WordEmbedder(vocab_size, embedding_dim)\n        self.positional_encoding = positional_encoding(longest_length, embedding_dim)\n        self.masked_attention = MaskedMultiHeadSelfAttention(heads, embedding_dim)\n        self.add_norm1 = AddNorm(embedding_dim)\n        self.attention = MultiHeadSelfAttention(heads, embedding_dim)\n        self.add_norm2 = AddNorm(embedding_dim)\n        self.feed_forward = FeedForward(embedding_dim)\n        self.add_norm3 = AddNorm(embedding_dim)\n        self.linear = nn.Linear(embedding_dim, vocab_size)\n\n    def forward(self, x):\n        x = self.word_embedder(x)\n        x = x + self.positional_encoding[:x.size(1), :].unsqueeze(0).expand(x.size(0), -1, -1)\n        x = self.add_norm1(x, self.masked_attention(x))\n        x = self.add_norm2(x, self.attention(x))\n        x = self.add_norm3(x, self.feed_forward(x))\n        logits = self.linear(x)\n        return F.softmax(logits, dim=-1)","metadata":{"id":"9fe1f5ed-35ce-479b-b596-d2cc95a2c06c","execution":{"iopub.status.busy":"2024-04-14T19:48:27.822669Z","iopub.execute_input":"2024-04-14T19:48:27.822955Z","iopub.status.idle":"2024-04-14T19:48:27.831998Z","shell.execute_reply.started":"2024-04-14T19:48:27.822933Z","shell.execute_reply":"2024-04-14T19:48:27.831086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Decoder(vocab_size, embedding_dim, batch_size, heads).to(device)","metadata":{"id":"881cd445-c7fb-4f2b-ae1f-d6807b2f76a5","execution":{"iopub.status.busy":"2024-04-14T19:48:27.833073Z","iopub.execute_input":"2024-04-14T19:48:27.833352Z","iopub.status.idle":"2024-04-14T19:48:27.889931Z","shell.execute_reply.started":"2024-04-14T19:48:27.833330Z","shell.execute_reply":"2024-04-14T19:48:27.889157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize our optimizer and loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()","metadata":{"id":"3026a026-a1e7-4f12-9f82-8a78982bb1ed","execution":{"iopub.status.busy":"2024-04-14T19:48:27.890974Z","iopub.execute_input":"2024-04-14T19:48:27.891247Z","iopub.status.idle":"2024-04-14T19:48:30.250263Z","shell.execute_reply.started":"2024-04-14T19:48:27.891225Z","shell.execute_reply":"2024-04-14T19:48:30.249314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(data_loader, model, optimizer, device=device):\n    model.train()\n    total_loss = 0\n\n    for batch in data_loader:\n        torch.cuda.empty_cache()\n        poems = batch[0].to(device)  # Assuming batch is correctly formatted\n        batch_size, seq_len = poems.size()\n\n        optimizer.zero_grad()\n\n        # Prepare input and target sequences\n        input_sequences = poems[:, :-1]  # Exclude the last token for input\n        target_sequences = poems[:, 1:]  # Exclude the first token for targets\n        # Forward pass using teacher forcing\n        logits = model(input_sequences)\n#         print(logits.transpose(1, 2).size())\n#         print(target_sequences.size())\n        loss = loss_fn(logits.transpose(1, 2), target_sequences)  # Adjust logits and targets format if necessary\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    average_loss = total_loss / len(data_loader)\n    return average_loss\n\nfor epoch in range(epochs):\n    epoch_loss = train_epoch(data_loader, model, optimizer)\n    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")","metadata":{"id":"bbfdc8bd-74e2-45f3-87ea-03c882aa2dee","outputId":"38e04aa9-52bc-4a25-de5e-769f64c3efd9","execution":{"iopub.status.busy":"2024-04-14T19:48:30.251465Z","iopub.execute_input":"2024-04-14T19:48:30.251873Z","iopub.status.idle":"2024-04-14T19:49:30.045519Z","shell.execute_reply.started":"2024-04-14T19:48:30.251849Z","shell.execute_reply":"2024-04-14T19:49:30.044261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_path = \"/kaggle/working/model_weights.pth\" \ntorch.save(model.state_dict(), model_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load(\"path/to/save/model_weights.pth\"))\nmodel.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_poem(model, start_sequence, max_length=500):\n    model.eval()  # Set the model to evaluation mode.\n    tokens = start_sequence.split()  # Tokenize the start sequence.\n    generated_sequence = [word_to_index.get(word, word_to_index[\"UNK\"]) for word in tokens]  # Convert to indices.\n\n    for _ in range(max_length):\n        input_seq = torch.tensor([generated_sequence], dtype=torch.long).to(device)\n        with torch.no_grad():\n            output_logits = model(input_seq)\n\n        # Predict the next word index.\n        next_word_id = output_logits[:, -1, :].argmax(-1).item()\n        generated_sequence.append(next_word_id)  # Append the index of the next word.\n\n        # Stop if the end token is generated.\n        if next_word_id == word_to_index[\"END\"]:\n            break\n\n    # Convert indices back to words.\n    generated_words = [index_to_word[idx] for idx in generated_sequence]\n\n    return ' '.join(generated_words)","metadata":{"id":"5132ae30-6205-4e12-a54e-b58b3f03e1bf","execution":{"iopub.status.busy":"2024-04-14T19:49:30.046538Z","iopub.status.idle":"2024-04-14T19:49:30.046871Z","shell.execute_reply.started":"2024-04-14T19:49:30.046708Z","shell.execute_reply":"2024-04-14T19:49:30.046721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_sequence = input(\"Please enter the starting words for your poem: \")\npoem = generate_poem(model, start_sequence, max_length=50)\nprint(poem)\n","metadata":{"id":"01b5e8f6-5146-4019-9f45-08c003279b49","execution":{"iopub.status.busy":"2024-04-14T19:49:30.048786Z","iopub.status.idle":"2024-04-14T19:49:30.049120Z","shell.execute_reply.started":"2024-04-14T19:49:30.048961Z","shell.execute_reply":"2024-04-14T19:49:30.048975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"23f509d4-499c-4267-928b-b0379cde2b36"},"execution_count":null,"outputs":[]}]}