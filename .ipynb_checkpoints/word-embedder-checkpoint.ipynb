{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113cc591-7e54-4fe2-a795-9c0e7d289c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_window_size):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.context_window_size = context_window_size\n",
    "        \n",
    "    def forward(self, titles):\n",
    "        batch_size, total_length = titles.shape \n",
    "        \n",
    "        # Initialize result tensor filled with padding value, here assumed as ones for simplicity\n",
    "        # You might choose a different value or method depending on how you handle padding in embeddings\n",
    "        result_embeddings = torch.ones(batch_size, total_length - (2 * context_window_size), embedding_dim)\n",
    "            \n",
    "        for batch in range(batch_size):\n",
    "            title = titles[batch]\n",
    "    \n",
    "            # Find indices that are not padding\n",
    "            non_pad_indices = (title != pad_index).nonzero().squeeze()\n",
    "            \n",
    "            for idx, i in enumerate(non_pad_indices):\n",
    "                if i - self.context_window_size < 0 or i + self.context_window_size >= total_length:\n",
    "                    # Skip positions where full context cannot be obtained due to padding\n",
    "                    continue\n",
    "                    \n",
    "                # Gather context indices, avoiding going out of bounds\n",
    "                context_indices_left = title[max(i - self.context_window_size, 0):i]\n",
    "                context_indices_right = title[i+1:min(i+1+self.context_window_size, total_length)]\n",
    "                context_indices = torch.cat((context_indices_left, context_indices_right), dim=0).unsqueeze(0)\n",
    "                \n",
    "                # Compute and store the average context embedding at the appropriate position\n",
    "                context_embedding = self.embeddings(context_indices).mean(dim=1)\n",
    "                result_embeddings[batch, i, :] = context_embedding\n",
    "        \n",
    "        return result_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96022e28-6a64-4dc3-98f4-23e099575670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 15  # Number of epochs to train the model\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        titles = batch[0].to(device)  # Move the batch of titles to the specified device (e.g., GPU)\n",
    "\n",
    "        optimizer.zero_grad()  # Reset the gradients to zero before the backward pass\n",
    "\n",
    "        # Compute the predicted context embeddings for the current batch\n",
    "        predicted_context_embeddings = word_embedder(titles)\n",
    "        # Prepare the target embeddings\n",
    "        # Assuming titles[:, context_window_size:-context_window_size] are the target indices\n",
    "        target_indices = titles[:, context_window_size:-context_window_size]\n",
    "        target_embeddings = word_embedder.embeddings(target_indices)\n",
    "        # The target embeddings have a shape of [batch_size, reduced_seq_len, embedding_dim]\n",
    "\n",
    "        # Create a mask to exclude padding and edge tokens from the loss calculation\n",
    "        pad_mask = (titles != pad_index)[:, context_window_size:-context_window_size].unsqueeze(-1)\n",
    "        pad_mask = pad_mask.expand(-1, -1, embedding_dim)  # Expand the mask to cover the embedding dimensions\n",
    "\n",
    "        # Apply the mask to the predicted and target embeddings\n",
    "        # This ensures that the loss is only computed for the relevant (non-padding) tokens\n",
    "        predicted_context_embeddings *= pad_mask\n",
    "        target_embeddings *= pad_mask\n",
    "\n",
    "        # Compute the loss between the predicted and target embeddings\n",
    "        # Note: Depending on the specific requirements, you may need to adjust the way the loss is computed\n",
    "        # to handle the 3D nature of the tensors and potential misalignment\n",
    "        loss = loss_fn(predicted_context_embeddings, target_embeddings)\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Compute the average loss for the current epoch\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
